                    Background        Microarray technology has revolutionized modern        biological research by permitting the simultaneous study of        genes comprising a large part of the genome The blessings        stemming from this are accompanied by the curse of high        dimensionality of the data output The main objective of        this article is to explore one method for ranking genes in        order of likelihood of being differentially expressed Top        gene lists that give few false positives and few false        negatives are the output As the interest is mainly in        ranking for the purpose of generating top gene lists        issues such as calculation of         p values and correction for multiple        tests are of secondary importance        Microarrays have an important role in finding novel drug        targets the thinking that guides the design and        interpretation of such experiments has been expressed by        Lonnstedt and Speed     The number of genes selected        would depend on the size aim background and followup        plans of the experiment Often interest is restricted to        socalled druggable target classes thus thinning out the        set of eligible genes considerably It is generally        sensible to focus attention first on druggable targets with        smaller         p values where the         p value is the probability of        obtaining at least the same degree of differential        expression by pure chance before proceeding to ones with        larger         p values In general         p values have the greatest impact on        decisions regarding target selection by providing a        preliminary ranking of the genes This is not to say that        multiplicity should never be taken into account or that        the method presented here replaces correction for        multiplicity On the contrary the approach provides a        basis for such calculations see Additional data        files        The approach presented here could be applied to        different types of test statistics but one particular type        of recently proposed statistic will be used In Tusher           a methodology based on a modified         t statistic is described                where         diff is an effect estimate for        example a group mean difference         S is a standard error and         S          is a regularizing constant This        formulation is quite general and includes for example the        estimation of a contrast in an ANOVA Setting         S            will yield a         t statistic The constant called        the fudge constant is found by removing the trend in         d as a function of         S in moving windows across the data        The technical details are outlined in     The statistic        calculated in this way will be referred to as SAM The        basic idea with         d is to eliminate some false        positives with low values of         S  see Figure         It is more relevant to optimize with respect to        falsepositive and falsenegative rates This is the basic        idea behind the new approach The idea is to jointly        minimize the number of genes that are falsely declared        positive and the number of genes falsely declared negative        by optimizing over a range of values of the significance        level a and the fudge constant         S           How well this is achieved can be        judged by a receiver operating characteristics ROC curve        which displays the number of false positives against the        number of false negatives expressed as proportions of the        total number of genes        An alternative to the statistic  is         d          diff          S                   S  or         d          diff          wS                     w          S  for some weight         w  which is basically the statistic        proposed in Baldi     Its performance appears to be        very similar to that of  data not shown A software        implementation in R code within the package SAG     is        available from    via the function         samroc                     Results                  The criterion          A comparison of methods in terms of their ROC curves          is presented in Lonnstedt     A method whose ROC          curve lies below another one has smaller ordinate for          given abscissa is preferred Figure  A method which          has a better ROC curve in this sense will produce top          lists with more differentially expressed genes DEGs          fewer nonDEGs and consequently will leave out fewer          DEGs Furthermore such a method will give higher average          ranks to the DEGs if the ranking is such that high rank          means more evidence of differential expression          Superiority in terms of average ranks is a weaker          assertion than superiority in terms of ROC curves see          Additional data files for a proof If it is desirable to          compare methods with respect to their ROC curves then          the estimation procedures should find parameter estimates          that optimize the ROC curve This section suggests a          goodness criterion based on the ROC curve          False discovery rate            FDR  may be defined as the          proportion of false positives among the significant          genes see     Falsepositive rate            FP  may be defined as the number          of false positives among the significant genes divided by          the total number of genes Similarly we define the          falsenegative rate            FN  as the number of false          negatives among the nonsignificant genes divided by the          total number of genes the truepositive rate            TP  as the number of true          positives divided by the total number of genes and the          truenegative rate            TN  as the number of true          negatives divided by the total number of genes          In Table relations involving these entities are          displayed For instance the proportion of unchanged          genes nonDEGs           p             equals the sum of the proportion          of true negative and the proportion of false positive           p                       TN            FP  and the proportion of          significant genes at a certain significance level           equals the sum of the true positives and the false          positives           p             TP            FP  It is intuitive that the          criterion to be minimized should be an increasing          function of           FP and           FN  Any top list produced should          have many DEGs and few nonDEGs          Assume that we can for every combination of values of          the significance level  and the fudge constant           S             calculate            FP FN  The goodness criterion is          then formulated in terms of the distance of the points            FP FN  to the origin which point          corresponds to no false positives and no false negatives          see Figure  which in mathematical symbols may be put          as                    The optimal value of            S             will be the one that minimizes           It is for practical reasons not possible to do this          minimization over every combination so the suggestion is          to estimate the criterion over a lattice of            S             values and pick the best          combination          If one has an assessment regarding the relative          importance of           FP and           FN  that may be reflected in a          version of the criterion  that incorporates a weight           that reflects the relative importance of           FP compared to           FN            C                         FP            FN  The choice               p                       p            corresponds to another type of ROC          curve which displays the proportion of true            TP             p             against the proportion of false                     FP            p             see Additional data files Other          goodness criteria are possible such as the sum of           FP and           FN or the area under the curve in          Figure  For more details and other approaches see for          example                               Calculating pvalues          Using the permutation method to simulate the null          distribution no change we can obtain a           p value for a twosided test as          detailed below Loosely speaking in each loop of the          simulation algorithm the group labels are randomly          rearranged so that random groups are formed the test          statistic is calculated for this arrangement and the          value is compared to the observed one How extreme the          observed test statistic is will be judged by counting the          number of times that more extreme values are obtained          from the null distribution          The data matrix                       X            has genes in rows and arrays in columns Consider          the vector of group labels fixed The permutation method          consists of repeatedly permuting the columns equivalent          to rearranging group labels thus obtaining the matrix                       X             and calculating the test statistic for each gene          and each permutation Let           d            j             k be the value of the statistic of          the           j th gene in the           k th permutation of columns Then          the           p value for gene           i equals                    where           M is the number of genes           d            i  the observed statistic for gene                    i            B the number of permutations and           denotes the cardinality of the set       In          words this gives the relative frequency of randomly          generated test statistics with an absolute value that          exceeds the observed value of gene           i  The formula  combines the          permutation method in    and the           p value calculation in              These           p values are such that a more          extreme value of the test statistic will yield a lower           p value          Given the significance level             p values less than  are          considered significant the proportion of the genes          considered differentially expressed is                    which is the relative frequency of genes with a           p value less than           The current version of           samroc uses the estimate                    where           q                       X            is the           X percentile of the           d  compare     This estimate          makes use of the fact that the genes whose test          statistics fall in the quartile range will be          predominantly the unchanged ones More material on this          matter is in the Additional data files                          Estimating FP          Going via results for the           FDR in Storey    see also               it is possible to derive the estimate                    which is the proportion of unchanged genes multiplied          by the probability that such a gene produces a          significant result For a derivation see the Additional          data files                          Estimating FN          From Table one obtains as outlined in the Additional          data files                    FN              p                                     p                        To get an intuitive feel for this equality just note          that the second term is the proportion unchanged          multiplied by the probability of such genes not being          significant which estimates           TN  and that the third term          corresponds to the positive            TP            FP  Subtracting the proportion of          these two categories from the whole will leave us with          the           FN                           Estimating the criterion          The entities we need for the optimisation are given by          the estimates                    and                    A scatter plot of the estimate of the criterion                    versus the true value is shown in Figure  and          reveals a good level of accuracy                          Tests                      Simulated cDNA data            The normal distributions modeled after reallife            cDNA data used in Baldi and Long    were used here            to provide a testing ground for the methods Table             In each simulation two groups of four arrays each were            created Three datasets with   and  DEGs were            generated using the normal distributions In all cases             samroc and the             t test coincided              S                and were the best methods in            terms of the ROC curves Theory predicts that the             t test is optimal in this            situation see Additional data files When data were            antilogarithmtransformed giving rise to lognormal            distributions             samroc again came out best            followed by the Bayes method The             t test falls behind this time            Figure gives a graphical presentation of the results            in terms of ROC curves                                Oligonucleotide leukemia data            The data on two types of leukemia ALL and AML            appeared in Golub             et al       Samples of            both types were hybridized to  arrays In                 genes were identified as DEGs using statistical            analysis of data from the full set of arrays For these            data it is impossible to calculate a ROC curve as the            DEGs are unknown Instead performance was assessed in            terms of the average rank of the  genes after all            genes were ranked by their likelihood of being DEGs            according to each of the methods Using just three            arrays from each of the two groups             samroc gave the best results            followed by SAM Table  This means that a necessary            but not sufficient condition for the superiority of             samroc in terms of ROC curves is            satisfied see Additional data files                                Affymetrix spiking experiment data            In this test data generated by Affymetrix in an            experiment where  transcripts were spiked at known            quantities Table      were used Using three            arrays from each of two groups of arrays where  probe            sets genes differ further datasets with  and             DEGs were generated by a bootstrap procedure Thus            there were three datasets with roughly   and             DEGs In two of these three settings             samroc performed best and in one            case  SAM and the Bayes method were better            Figure gives a graphical presentation of these results            in terms of ROC curves                                      Discussion        Whether to look at data on a log scale or not is a        tricky question and is beyond the scope of this article        However the best performance by the tests considered was        achieved when data were lognormal see Additional data        files Normal lognormal and reallife data were all        included in order to supply a varied testing ground        As pointed out in     the Bayes statistic is for        ranking purposes equivalent to a penalized         t statistic         t                   p                    mean                   mean                   a                   S  Here         a          is a scale parameter related to the         a priori distribution of the standard        error This means that it is at least in form closely        related to the         t test SAM and         samroc  SAM on the other hand        chooses as its fudge constant the value among the        percentiles of         S  which minimizes the coefficient        of variation of the median absolute deviation of the test        statistic computed over a number of percentiles of         S     It is interesting to note        how different the three related statistics the Bayes        method SAM and         samroc turn out in practice        One clue to why this difference occurs emerges when        comparing the denominators of SAM         samroc and Bayes more closely First        square the denominators of  and the representation of        Bayes above We obtain          a          S           a           aS          S for  and         a                   S for Bayes where generally         a                   a           For large values of         S the former will exceed the latter        This means that SAM         samroc will downplay the importance        of the results for high expressing genes in a way that the        Bayes method does not        But there is also another difference The Bayes method        seems to achieve best when the number of false positives is        allowed to grow rather large The constant         a corresponds to a large percentile        in the distribution of the         S values see Additional data        files Whereas the constant in SAM will generally be        rather small often the  percentile of the         S values the constant in the Bayes        method will correspond to at least the  percentile of        the         S values It seems that using a        large percentile will give a good performance when the        number of false positives grows large This observation is        consistent with the observation made in Lonnstedt and Speed           that the particular version of SAM which always uses        the  percentile will pass the Bayes method when the        number of false positives is allowed to grow large Also         samroc will in general make use of a        smaller percentile albeit that         samroc shows greater spread between        datasets in the values chosen as a result of its        adaptation to the features specific to the data at        hand        Samroc is the only method that makes explicit use of the        number of changed genes in the ranking If one has reason        to believe for example from studying expression  that        there are very few DEGs   then         samroc is probably not the first        choice Probably SAM or the Bayes method is more useful in        these situations If on the other hand the number of DEGs        is reasonably large         samroc is conjectured to take        precedence over SAM and to be more robust than the Bayes        method Furthermore one can argue that the kind of        experiments undertaken in drug discovery would more often        than not end in comparisons in which the biological systems        show vast differences in a large number of genes mostly as        a downstream effect of some shock to the system        The proposed method comes out better than or as good as        the original SAM statistic in most tests performed The         samroc statistic is robust and        flexible in that it can address all sorts of problems that        suit a linear model The methodology adjusts the fudge        constant flexibly and achieves an improved performance The        algorithm gives fewer false positives and fewer false        negatives in many situations and was never much worse than        the best test statistic in any circumstance However a        typical run with reallife data will take several hours on        a desktop computer To make this methodology better suited        for production it would be a good investment to translate        part of the R code or the whole of it into C        To improve on standard univariate tests one must make        use of the fact that data are available on a large number        of related tests One way of achieving this goal has been        shown in this paper The conclusion is that it is possible        and sensible to calibrate the test with respect to        estimates of the falsepositive and falsenegative        rates                    Additional data files        A zip file Additional data file  containing the R        package SAG for retrieval preparation and analysis of data        from the Affymetrix GeneChip and the R script Additional        data file  are available with the online version of this        article An appendix Additional data file  giving        further details of the statistical methods and the         samroc algorithm is also available as        a PDF file        Additional data file         A zip file containing the R package SAG for retrieval        preparation and analysis of data from the Affymetrix        GeneChip        A zip file containing the R package SAG for retrieval        preparation and analysis of data from the Affymetrix        GeneChip        Click here for additional data file        Additional data file         The R script        The R script        Click here for additional data file        Additional data file         An appendix giving further details of the statistical        methods and the         samroc algorithm        An appendix giving further details of the statistical        methods and the         samroc algorithm        Click here for additional data file            