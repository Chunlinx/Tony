                    Rationale        The information held in genomic sequence is encoded and        highly compressed to extract biologically interesting data        we must decrypt this primary data computationally This        generates results that provide a measure of biologically        relevant characteristics such as coding potential or        sequence similarity present in the sequence Because of        the amount of sequence to be examined and the volume of        data generated these results must be automatically        processed and carefully filtered        There are essentially three different strategies for        wholegenome analysis The first is a purely automatic        synthesis from a combination of analyses to predict gene        models The second aggregates analyses contributed by the        research community that the user is then required to        integrate visually on a public website The third is        curation by experts using a full trail of evidence to        support an integrated assessment Several groups charged        with rapidly providing a dispersed community with genome        annotations have chosen the purely computational route        examples are Ensembl   and the National Center for        Biotechnology Information NCBI   Approaches using        aggregation adapt well to the dynamics of collaborative        groups which are focused on sharing results as they accrue        examples are the University of California Santa Cruz UCSC        genome browser   and the Distributed Annotation System        DAS   For organisms with well established and        cohesive communities the demand is for carefully reviewed        and qualified annotations this approach was adopted by        three of the oldest genomecommunity databases SGD for         Saccharomyces cerevisiae   ACeDB        for         Caenorhabditis        elegans documentation code and data available from        anonymous FTP servers at   and FlyBase for         Drosophila melanogaster          We decided to examine every gene and feature of the         Drosophila genome and manually        improve the quality of the annotations   The        prerequisites for this are first a computational pipeline        and a database capable of both monitoring the pipelines        progress and storing the raw analysis second an        additional database to provide the curators with a        complete compact and salient collection of evidence and to        store the annotations generated by the curators and third        an editing tool for the curators to create and edit        annotations based on this evidence This paper discusses        our solution for the first two requirements The editing        tool used Apollo is described in an accompanying paper                 Our primary design requirement was flexibility This was        to ensure that the pipeline could easily be tuned to the        needs of the curators We use two distinct databases with        different schemata to decouple the management of the        sequence workflow from the sequence annotation data itself        Our longterm goal is to provide a set of opensource        software tools to support largescale genome        annotation                    Sequence datasets        The sequence datasets are the primary input into the        pipeline These fall into three categories the         D melanogaster genomic sequence        expressed sequences from         D melanogaster  and informative        sequences from other species        Release  of the         D melanogaster genomic sequence was        generated using bacterial artificial chromosome BAC        clones that formed a complete tiling path across the        genome as well as wholegenome shotgun sequencing reads          This genomic sequence was frozen when during        sequence finishing there was sufficient improvement in the        quality to justify a new release This provided a stable        underlying sequence for annotation        In general the accuracy and scalability of        geneprediction and similaritysearch programs is such that        computing on  million base Mb chromosome arms is        illadvised and we therefore cut the finished genomic        sequence into smaller segments Ideally we would have        broken the genome down into sequence segments containing        individual genes or a small number of genes Before the        first round of annotation however this was not possible        for the simple reason that the position of the genes was as        yet unknown Therefore we began the process of annotation        using a nonbiological breakdown of the sequence We        considered two possibilities for the initial sequence        segments either individual BACs or the segments that        comprise the public database accessions We rejected the        use of individual BAC sequences and chose to use the        GenBank accessions as the main sequence unit for our        genomic pipeline because the BACs are physical clones with        physical breaks while the GenBank accession can        subsequently be refined to respective biological entities        At around  kilobases kb these are manageable by most        analysis programs and provide a convenient unit of work for        the curators To minimize the problem of genes straddling        these arbitrary units we first fed the BAC sequences into a        lightweight version of the full annotation pipeline that        estimated the positions of genes We then projected the        coordinates of these predicted genes from the BAC clones        onto the fullarm sequence assembly This step was followed        by the use of another inhouse software tool to divide up        the arm sequence trying to simultaneously optimize two        constraints to avoid the creation of gene models that        straddle the boundaries between two accessions and to        maintain a close correspondence to the preexisting Release         accessions in GenBankEMBLDDBJ     During the        annotation process if a curator discovered that a unit        broke a gene they requested an appropriate extension of        the accession prior to further annotation In hindsight we        have realized that we should have focused solely on        minimizing gene breaks because further adjustments by        GenBank were still needed to ensure that as much as        possible genes remained on the same sequence        accession        To reannotate a genome in sufficient detail an        extensive set of additional sequences is necessary to        generate sequence alignments and search for homologous        sequences In the case of this project these sequence        datasets included assembled fullinsert cDNA sequences        expressed sequence tags ESTs and cDNA sequence reads        from         D melanogaster as well as peptide        cDNA and EST sequences from other species The sequence        datasets we used are listed in Figure and described more        fully in                      Software for task monitoring and scheduling the        computational pipeline        There are three major infrastructure components of the        pipeline the database the Perl module named Pipeline        and sufficient computational power allocated by a        jobmanagement system The database is crucial because it        maintains a persisting record reflecting the current state        of all the tasks that are in progress Maintaining the        jobs job parameters and job output in a database avoids        some of the inherent limitations of a filesystem approach        It is easier to update provides a builtin querying        language and offers many other datamanagement tools that        make the system more robust We used a MySQL   database        to manage the large number of analyses run against the        genome transcriptome and proteome see below        MySQL is an opensource structured query language        SQL database that despite having a limited set of        features has the advantage of being fast free and simple        to maintain SQL is a database query language that was        adopted as an industry standard in  An SQL database        manages data as a collection of tables Each table has a        fixed set of columns also called fields and usually        corresponds to a particular concept in the domain being        modeled Tables can be crossreferenced by using primary        and foreign key fields The database tables can be queried        using the SQL language which allows the dynamic        combination of data from different tables   A        collection of these tables is called a database schema and        a particular instantiation of that schema with the tables        populated is a database The Perl modules provide an        application programmer interface API that is used to        launch and monitor jobs retrieve results and support other        interactions with the database        There are four basic abstractions that all components of        the pipeline system operate upon a sequence a job an        analysis and a batch A sequence is defined as a string        of amino or nucleic acids held either in the database or as        an entry in a FASTA file usually both A job is an        instance of a particular program being run to analyze a        particular sequence for example running BLASTX to compare        one sequence to a peptide set is considered a single job        Jobs can be chained together If job A is dependent on the        output of job B then the pipeline software will not launch        job A until job B is complete This situation occurs for        example with programs that require masked sequence as        input An analysis is a collection of jobs using the same        program and parameters against a set of sequences Lastly        a batch is a collection of analyses a user launches        simultaneously Jobs analyses and batches all have a        status attribute that is used to track their progress        through the pipeline Figure         The three applications that use the Perl API are the        pipelauncher script the flyshell interactive command line        interpreter and the internet front end   Both        pipelauncher and flyshell provide pipeline users with a        variety of powerful ways to launch and monitor jobs        analyses and batches These tools are useful to those with        a basic understanding of Unix and bioinformatics tools as        well as those with a good knowledge of objectoriented        Peri The web front end is used for monitoring the progress        of the jobs in the pipeline        The pipelauncher application is a commandline tool        used to launch jobs Users create configuration files that        specify input data sources and any number of analyses to be        performed on each of these data sources along with the        arguments for each of the analyses Most of these        specifications can be modified with command line options        This allows each user to create a library of configuration        files for sending off large batches of jobs that can be        altered with commandline arguments when necessary        Pipelauncher returns the batch identifier generated by the        database to the user To monitor jobs in progress the        batch identifier can be used in a variety of commands such        as monitor batch deletebatch and        querybatch        The flyshell application is an interactive commandline        Perl interpreter that presents the database and pipeline        APIs to the end user providing a more flexible interface        to users who are familiar with objectoriented Perl        The web front end allows convenient browserbased        access for end users to follow the status of analyses An        HTML form allows users to query the pipeline database by        job analysis batch or sequence identifier The user can        drill down through batches and analyses to get to        individual jobs and get the status raw job output and        error files for each job This window on the pipeline has        proved a useful tool for quickly viewing results        Once a program has successfully completed an analysis of        a sequence the pipeline system sets its job status in the        database to FIN Figure  The raw results are recorded in        the database and may be retrieved through the web or Perl        interfaces The raw results are then parsed filtered and        stored in the database and the jobs status is set to        PROCD At this point a GAME Genome Annotation Markup        Elements XML extensible Markup Language          representation of the processed data can be retrieved        through either the Perl or web interfaces                    Analysis software                  Simwrap          Sim   is a highly useful and largely accurate way          of aligning fulllength cDNA and EST sequences against          the genome   Sim is designed to align nearly          identical sequences and if dissimilar sequences are used          the results will contain many errors and the execution          time will be long To circumvent this problem we split          the alignment of           Drosophila cDNA and EST sequences          into two serial tasks and wrote a utility program          simwrap to manage these tasks Simwrap executes a          first pass using BLASTN using the genome sequence as the          query sequence and the cDNA sequences as the subject          database We run BLASTN    with the B  option          as we are only interested in the summary part of the          BLAST report not in the highscoring pairs HSPs          portion where the alignments are shown From this BLAST          report summary simwrap parses out the sequence          identifiers and filters the original database to produce          a temporary FASTA data file that contains only these          sequences Finally we run sim again using the genomic          sequence as the query and the minimal set of sequences          that we have culled as the subject                          Autopromote          The           Drosophila genome was not a blank          slate because there were previous annotations from the          Release  genomic sequence   Therefore before the          curation of a chromosome arm began we first          autopromoted the Release  annotations and certain          results from the computational analyses to the status of          annotations This simplified the annotation process by          providing an advanced starting point for the curators to          work from          Autopromotion is not a straightforward process First          there have been significant changes to the genome          sequence between releases Second all of the annotations          present in Release  must be accounted for even if          ultimately they are deleted Third the autopromotion          software must synthesize different analysis results some          of which may be conflicting Autopromote resolves          conflicts using graph theory and voting networks          Table lists the programs and parameters that were          used for the analysis of the genomic sequence and peptide          analysis                            Berkeley Output Parser filtering                  Sim filtering          Our primary objective in using sim was to align           Drosophila ESTs and cDNA sequences          only to the genes that encoded them and not to          genefamily members and for this reason we applied          stringent measures before accepting an alignment For          sim the filtering parameters are as follows                    Score is the minimum percent          identity that is required to retain an HSP or alignment          the default value is                     Coverage is a percentage of the          total length of the sequence that is aligned to the          genome sequence Any alignments that are less than this          percentage length are eliminated we required  of the          length of a cDNA to be aligned                    Discontinuity sets a maximum gap          length in the aligned EST or cDNA sequence The primary          aim of this parameter is to identify and eliminate          unrelated sequences that were physically linked by a cDNA          library construction artifact                    Remove polyA tail is a Boolean to          indicate that short terminal HSPs consisting primarily of          runs of a single base either T or A because we could not          be certain of the strand are to be removed                    Join  and  is a Boolean          operation and is used for EST data If it is true BOP          will do two things First BOP will reverse complement          any hits where the name of the sequence contains the          phrase prime Second it will merge all alignments          where the prefixes of the name are the same Originally          this was used solely for the  and  ESTs that were          available However when we introduced the internal          sequencing reads from the           Drosophila Gene Collection DGC          cDNA sequencing project   into the pipeline this          portion of code became an alternative means of          effectively assembling the cDNA sequence Using the          intersection of each individual sequence alignment with          the genome sequence a single virtual cDNA sequence was          constructed          Another tactic for condensing primary results without          removing any information is to reconstruct all logically          possible alternative transcripts from the raw EST          alignments by building a graph from a complete set of          overlapping ESTs Each node comprises the set of spans          that share common splice junctions The root of the graph          is the node with the most  donor site It is of          course also possible to have more than one starting          point for the graph if there are overlapping nodes with          alternative donor sites The set of possible transcripts          is the number of paths through this trees This          analysis produced an additional set of alignments that          augmented the original EST alignments                          External pipelines          Of the numerous geneprediction programs available we          incorporated only two into our pipeline This was because          some of these programs are difficult to integrate into a          pipeline some are highly computationally expensive and          others are only available under restricted licenses          Rather than devoting resources to running an          exhaustive suite of analyses we asked a number of          external groups to run their pipelines on our genomic          sequences We received results for three of the five          chromosome arms L R R from Celera Genomics          Ensembl and NCBI pipelines These predictions were          presented to curators as extra analysis tiers in Apollo          and were helpful in suggesting where coding regions were          located However in practice human curators required          detailed alignment data to establish biologically          accurate gene structures and this information was only          available from our internal pipeline                          Hardware          As an inexpensive solution to satisfy the          computational requirements of the genomic analyses we          built a Beowulf cluster   and utilized the portable          batch system PBS software developed by NASA   for          job control A Beowulf cluster is a collection of          processor nodes that are interconnected in a network and          the sole purpose of these nodes and the network is to          provide processor compute cycles The nodes themselves          are inexpensive offtheshelf processor chips connected          using standard networking technology and running          opensource software when combined these components          generate a lowcost highperformance compute system Our          nodes are all identical and use Linux as their base          operating system as is usual for Beowulf clusters          The Beowulf cluster was built by Linux NetworX            which also provided additional hardware ICE box and          Clusterworx software to install the system software and          control and monitor the hardware of the nodes The          cluster configuration used in this work consisted of           standard IA architecture nodes each with dual Pentium          III CPUs running at  MHz GHz and  MB memory In          addition a single Pentium IIIbased master node was used          to control the cluster nodes and distribute the compute          jobs Nodes were interconnected with standard BT          Ethernet on an isolated subnet with the master node as          the only interface to the outside network The private          cluster  BT network was connected to the NASbased          storage volumes housing the data and user home          directories with Gigabit ethernet Each node had a  GB          swap partition used to cache the sequence databases from          the network storage volumes To provide a consistent          environment the nodes had the same mounting points of          the directories as all other BDGP Unix computers The          networkwide NIS maps were translated to the internal          cluster NIS maps with an automated script Local hard          disks on the nodes were used as temporary storage for the          pipeline jobs          Job distribution to the cluster nodes was done with          the queuing system OpenPBS version    PBS was          configured with several queues and each queue having          access to a dynamically resizable overlapping fraction of          nodes Queues were configured to use one node at a time          either running one job using both CPUs such as the          multithreaded BLAST or Interpro motif analysis or two          jobs using one CPU each for optimal utilization of the          resources Because of the architecture of the pipeline          individual jobs were often small but tens of thousands of          them may be submitted at any given time Because the          default PBS firstinfirstout FIFO scheduler while          providing a lot of flexibility does not scale up beyond          about  jobs per queue the scheduler was          extended With this extension the scheduler caches jobs          in memory if a maximum queue limit is exceeded Job          resource allocation was managed on a per queue basis          Individual jobs could only request cluster resources          based on the queue they were submitted to and each queue          was run on a strict FIFO basis With those modifications          PBS was scaled to over  jobs while still          permitting higherpriority jobs to be submitted to a          separate highpriority queue                            Storing and querying the annotation results the        Gadfly database        A pipeline database is useful for managing the execution        and postprocessing of computational analyses The end        result of the pipeline process is streams of prediction and        alignment data localized to genomic transcript or peptide        sequences We store these data in a relational database        called Genome Annotation Database of the Fly Gadfly        Gadfly is the second of the two database schemata used by        the annotation system and will be discussed elsewhere        We initially considered using Ensembl as our sequence        database At the time we started building our system        Ensembl was also in an early stage of development We        decided to develop our own database and software while        trying to retain interoperability between the two This        proved difficult and the two systems diverged While this        was wasteful in terms of redundant software development it        did allow us to hone our system to the particular needs of        our project Gadfly remains similar in architecture and        implementation details to Ensembl Both projects make use        of the bioPerl bioinformatics programming components                   The core data type in Gadfly is called a sequence        feature This can be any piece of data of biological        interest that can be localized to a sequence These roughly        correspond to the types of data found in the feature        table summary of a GenBank report Every sequence feature        has a feature type  examples of feature types are        exon transcript proteincoding gene tRNA gene        and so on        In Gadfly sequence features are linked together in        hierarchies For instance a gene model is linked to the        different transcripts that are expressed by that gene and        these transcripts are linked to exons Gadfly does not        store some sequence features such as introns or        untranslated regions UTR as this data can be inferred        from other features Instead Gadfly contains software rules        for producing these features on demand        Sequence features can have other pieces of data linked        to them Examples of the kind of data we attach are        functional data such as Gene Ontology GO   term        assignments tracking data such as symbols synonyms and        accession numbers data relevant to the annotation process        such as curator comments   data relevant to the        pipeline process such as scores and expectation values in        the case of computed features Note that there is a wealth        of information that we do not store particularly genetic        and phenotypic data as this would be redundant with the        FlyBase relational database        A core design principle in Gadfly is flexibility using        a design principle known as generic modeling We do not        constrain the kinds of sequence features that can be stored        in Gadfly or constrain the properties of these features        because our knowledge of biology is constantly changing        and because biology itself is often unconstrained by rules        that can be coded into databases As much as possible we        avoid builtin assumptions that if proven wrong would        force us to revisit and explicitly modify the software that        embodies them        The generic modeling principle has been criticized for        being too loosely constrained and leading to databases that        are difficult to maintain and query This is a perceived        weakness of the ACeDB database We believe we have found a        way round this by building the desired constraints into the        program components that work with the database we are also        investigating the use of ontologies or controlled        vocabularies to enforce these constraints A detailed        discussion of this effort is outside the scope of this        paper and will be reported elsewhere        Figure shows the data flow in and out of Gadfly        Computational analysis features come in through analysis        pipelines  either the Pipeline via BOP or through an        external pipeline usually delivered as files conforming to        some standardized bioinformatics format for example GAME        XML GFF        Data within Gadfly is sometimes transformed by other        Gadfly software components For instance just before        curation of a chromosome arm commences different        computational analyses are synthesized into best guesses        of gene models as part of the autopromote software we        described earlier        During the creation of Release  annotations curators        requested data from Gadfly by specifying a genomic region        Although this region can be of any size we generally        allocated work by GenBank accessions Occasionally        curators worked one gene at a time by requesting genomic        regions immediately surrounding the gene of interest        Gadfly delivers a GAME XML file containing all of the        computed results and the current annotations within the        requested genomic region The curator used the Apollo        editing tool to annotate the region after which the data        in the modified XML file was stored in Gadfly        The generation of a highquality set of predicted        peptides is one of our primary goals To achieve this goal        we needed a means of evaluating the peptides and presenting        this assessment to the curators for inspection so that        they might improve the quality of the predicted peptides        iteratively Every peptide was sent through a peptide        pipeline to assess the predicted peptide both        quantitatively and qualitatively Where possible we wanted        to apply a quantifiable metric requiring a standard        against which we could rate the peptides For this purpose        we used peptides found in SPTRREAL E Whitfield personal        communication a carefully reviewed database of published        peptide sequences for comparison to our predicted        proteins SPTRREAL is composed of          D melanogaster sequences from the        SWISSPROT and TrEMBL protein databases   and provides        a curated proteinsequence database with a high level of        annotation a minimal level of redundancy and the absence        of any hypothetical or computational gene models Our        program PEPQC performed this crucial aspect of the        annotation process and is described below In cases where a        known peptide was unavailable we used a qualitative        measure to evaluate the peptide The peptide pipeline        provided a BLASTP analysis with comparisons to peptides        from other model organism genome sequences and InterProScan          analysis for proteinfamily motifs to enable the        curators to judge whether the biological properties of the        peptide were reasonable        Each annotation cycle on a sequence may affect the        primary structure of the proteins encoded by that sequence        and these changes must therefore trigger a reanalysis of        the edited peptides Whereas the genomic pipeline is        launched at distinct stages on an armbyarm basis the        peptide pipeline is run whenever a curator changes a gene        model and saves it to the Gadfly database To rapidly        identify whether the peptide sequence generated by the        altered gene model has also changed the database uniquely        identifies every peptide sequence by its name and its MD        checksum   The MD checksum provides a fast and        convenient way of determining whether two sequences are        identical To determine whether a peptide sequence has been        altered is a simple comparison of the prior checksum to the        new checksum allowing us to avoid using compute cycles in        reanalyzing sequences that have not changed        PEPQC generates both summary status codes and detailed        alignment information for each gene and each peptide        ClustalW   and showalign   are used to generate a        multiple alignment from the annotated peptides for the gene        and the corresponding SPTRREAL peptide or peptides In        addition brief discrepancy reports are generated that        describe each SPTRREAL mismatch clearly For instance an        annotated peptide might contain any or all of the        mismatches in Table in this example CGPB is the        initial FlyBase annotation and QX is the SPTRREAL        entry        The quality assessments produced by the peptide pipeline        need to be available to the curators for inspection during        annotation sessions so that any corrections that are needed        can be made Curators also need to access other relevant        FlyBase information associated with a gene in order to        refine an annotation efficiently We developed        automatically generated minigenereports to consolidate        this gene data into a single web page Minigenereports        include all the names and synonyms associated with a gene        its cytological location and accessions for the genomic        sequence ESTs PIR records and Drosophila Gene Collection          assignments if any All of these items are        hyperlinked to the appropriate databases for easy access to        more extensive information All literature references for        the gene appear in the reports with hyperlinks to the        complete text or abstracts The minigenereports also        consolidate any comments about the gene including        amendments to the gene annotation submitted by FlyBase        curators or members of the         Drosophila community The        minigenereports can be accessed directly from Apollo or        searched via a web form by gene name symbol synonym        including the FlyBase unique identifier or FBgn or        genomic location A web report grouped by genomic segment        and annotator is updated nightly and contains lists of        genes indexed by status code and linked to their individual        minigenereports                    Other integrity checks        Before submission to GenBank a number of additional        checks are run to detect potential oversights in the        annotation These checks include confirming the validity of        any annotations with open reading frames ORF that are        either unusually short less than  amino acids or less        than  of the transcript length In the special case of        known small genes such as the         Drosophila immune response genes        DIRGs   the genome annotations are scanned to ensure        that no welldocumented genes have been missed Similarly        the genome is scanned for particular annotations to verify        their presence including those that have been submitted as        corrections from the community or are cited in the        literature such as tRNA snRNA snoRNA microRNA or rRNA        genes documented in FlyBase If the translation start site        is absent an explanation must be provided in the comments        Annotations may also be eliminated if annotations with        different identifiers are found at the same genome        coordinates or if a proteincoding gene overlaps a        transposable element or a tRNA overlaps a proteincoding        gene Conversely duplicated gene identifiers that are        found at different genome coordinates are either renamed or        removed A simple syntax check is also carried out on all        the annotation symbols and identifiers Known mutations in        the sequenced strain are documented and the wildtype        peptide is submitted in place of the mutated version        The BDGP also submits to GenBank the cDNA sequence from        the DGC project Each of these cDNA clones represents an        expressed transcript and it is important to the community        that the records for these cDNA sequences correctly        correspond to the records for the annotated transcripts in        both GenBank and FlyBase This correspondence is        accomplished via the cDNA sequence alignments to the genome        described previously After annotation of the entire genome        was completed these results were used to find the        intersection of cDNA alignments and exons A cDNA was        assigned to a gene when the cDNA overlapped most of the        gene exons and the predicted peptides of each were verified        using a method similar to PEPQC                    Public World Wide Web interface        We provide a website for the community to query Gadfly        This allows queries by gene by genomic or map region by        Gene Ontology GO assignments or by InterPro domains As        well as delivering humanreadable web pages we also allow        downloading of data in a variety of computerreadable        formats supported by common bioinformatics tools We use        the GBrowse   application which is part of the GMOD          collection of software for visualization and        exploration of genomic regions                    Software engineering        The main software engineering lesson we learned in the        course of this project was the importance of flexibility        Nowhere was this more important than in the database        schema In any genome normal biology conspires to break        carefully designed data models Among the examples we        encountered while annotating the         D melanogaster genome were the        occurrence of distinct transcripts with overlapping UTRs        but nonoverlapping coding regions leading us to modify        our original definition of alternative transcript the        existence of dicistronic genes two or more distinct and        nonoverlapping coding regions contained on a single        processed mRNA requiring support for one to many        relationships between transcript and peptides and         trans splicing exhibited by the         mod          mdg  gene   requiring a new        data model We also needed to adapt the pipeline to        different types and qualities of input sequence For        example to analyze the draft sequence of the repeatrich        heterochromatin   we needed to adjust the parameters        and datasets used but also to develop an entirely new        repeatmasking approach to facilitate gene finding in        highly repetitive regions We are now in the process of        modifying the pipeline to exploit comparative genome        sequences more efficiently Our intention is to continue        extending the system to accommodate new biological research        situations        Improvements to tools and techniques are often as        fundamental to scientific progress as new discoveries and        thus the sharing of research tools is as essential as        sharing the discoveries themselves We are active        participants in and contributors to the Generic Model        Organism Database GMOD project which seeks to bring        together opensource applications and utilities that are        useful to the developers of biological and genomic        databases We are contributing the software we have        developed during this project to GMOD Conversely we reuse        the Perlbased software GBrowse from GMOD for the visual        display of our annotations        Automated pipelines and the management of downstream        data require a significant investment in software        engineering The pipeline software the database and the        annotation tool Apollo as a group provide a core set of        utilities to any genome effort that shares our annotation        strategy Exactly how portable they are remains to be seen        as there is a tradeoff between customization and ease of        use We will only know the extent to which we were        successful when other groups try to reuse and extend these        software tools Nevertheless the wealth of experience we        gained as well as the tools we developed in the process of        reannotating the         Drosophila genome will be a valuable        resource to any group wishing to undertake a similar        exercise            