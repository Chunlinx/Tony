                    Background        Most health statistics are reported with an explicit        quantification of uncertainty because they are based on a        sample from a target population possibly with random        assignment of treatments and quantifying the resulting        stochastic error is done almost universally Extrapolations        from samples are not however the only way to calculate        rates totals or other quantitative measures of health        The availability of data may lead to an approach that does        not involve sampling or any other random process For        example         Automobile fatality totals are typically computed with        an attempt to completely enumerate counting every        case         Two states might create a natural experiment by        having different traffic or safety regulations Differences        between or ratios of frequencies of accidents or injuries        could then be computed by enumeration and arithmetic         Samples of convenience are extrapolated to the entire        population such as trying to impute the US incidence of         Escherichia coli OH infections        based on data from the few states that report good data        While this is a sample the error comes not from random        sampling error which would be quite small but the other        sources identified below         To estimate the rate of a disease in a community we        frequently reverse this process and interpolate from a        national average         Trends may be inferred from data that is believed to        be related to the measure of interest but in unknown ways        such as tracking the effect of economic changes on mental        health by tracking the number of calls to hotlines        Observational studies and randomized trials almost        always quantify error due to random sampling and        allocation The realization which was close to universal        as long as four decades ago that health research results        need to include such quantification has helped reduce the        frequency of conclusions based on inadequate sample sizes        However the ease of quantifying that single source of        error has distracted researchers from the many other often        larger errors in any estimated quantity Quantification of        the one source of error implies that this represents all        uncertainty This implication grossly overstates precision        by ignoring uncorrected confounding selection biases        measurement errors and specification of functional        relationships       This is especially clear when a        calculation does not involve sampling and lacking the one        source of error that we commonly quantify the numbers are        reported as point estimates with no acknowledgment of        uncertainty at all A complete lack of error quantification        implies the even more misleading claim that the result is        perfect While some readers recognize the inevitable        uncertainty and guess at its magnitude most do not        Two highly publicized recent examples illustrate this        The death counts from the September  attacks on New York        were updated hourly and reported to four significant        figures the exact count But the reports from the first        few weeks turned out to be high by a factor of two making        it quite clear that even the apparently precise counting of        fatalities from a single event can only be estimated within        a range of error The vote count in Florida in the         US presidential election involved complete enumeration        People were shocked that there was so much uncertainty         due to measurement and recording errors among other things         in what they imagined to be a flawless mechanistic        process Few people understood that the results from the        various counts represented a statistical tie and that        choosing which vote count was the right one was a matter        of legalistic detail rather than scientific truth Note        that this considers only the votes as counted The illegal        disenfranchisement of tens of thousands of eligible voters         who would have almost certainly broken the tie  reminds        us that uncorrected systematic bias can have much larger        magnitude than the measured result                        Analysis and discussion                  A simple method for quantifying errors in simple          statistics          A quick and easy way to avoid overstating precision is          appropriate rounding as taught to high school science          students and largely ignored in health science reports          though a brief exposition of the point can be found in a          recent epidemiology textbook      p This          method is rough and thus not perfectly welldefined          but it is a fairly effective shorthand do not report          significant digits ie digits other than placeholder          zeros beyond the level of precision of your estimate If          your point estimate for some value is  but you          think it is fairly likely that the true value is lower or          higher by as much as  only report  This can be          interpreted as roughly we are pretty sure the result is          between  and  but cannot be much more precise          Similarly if your estimate is  but you know the          measurement is only precise to plusorminus five          thousand report           The limits of this method are clear when you consider          what to report in the first example if you want to          reflect confidence of plusorminus  Reporting           implies a bit too much precision but reporting  implies          too little It usually makes sense to imply a bit too          much precision rather than too little thus providing          more information about the point estimate but we should          stop at the minimum level of overprecision possible           in this case and not imply more precision still eg          by reporting           Annual US automobile accident fatalities are          reported to five figures eg  for               but when presenting this result for most purposes it is          better to report  roughly estimating the          limitations of measurement eg some deaths should be          counted as suicides or were fatal cardiovascular events          before the crash and record keeping eg cases          inadvertently recorded and reported by two different          jurisdictions          Notwithstanding the lack of a perfect rule it should          be clear when a result is presented with far too much          precision as is often the case One of the most          influential epidemiologic publications of recent years          Kernan et als    study of phenylpropanolamine and          stroke that resulted in that popular decongestant and          diet aid being removed from the US market reported one          odds ratio of  even though one of the cell counts          generating that ratio exposed noncases was exactly           and thus it could only be precise to about  part in           not  in  Consider that if the cell count differed          by the minimum possible  the odds ratio would either          be cut in half or increased to infinity It is difficult          to assess exactly what impact this misleading claim of          precision had on policy makers and other readers but we          might suspect that it contributed to the unwarranted          confidence in the studys findings          If a more formal quantification of uncertainty is          reported  such as reporting  plusorminus           for the above example  then the significant digits are          no longer the only reporting of uncertainty and are not          so important Nevertheless if  appears in a paper          there is a risk that it will be repeated out of context          in all its implicit precision without the           clarification          It should be noted that rounding to an appropriate          number of significant digits or any other method in this          paper does not           create any imprecision the          imprecision exists even if we do not accurately report          it                          Improved quantification of errors in simple          statistics                      Example            A report presented to the public and policy makers            states that  percent of the people in a community            have been diagnosed with a certain disease during a            oneyear period based on active monitoring that            identified  people diagnosed out of a community of             Because the study method was a complete            enumeration there is no random process and thus no            frequentist error statistics The resulting lack of a            confidence interval means that no statement of error            accompanied the result It is certain however that            there is still error In particular the researchers            believe that  is an undercount of as much as             due to the likely inability of the monitoring system to            detect all cases            The total population of the community is also            uncertain but this is inconsequential by the above            definition If reporting the figure in a final report            it would probably be appropriate to report             rather than the six figures but this uncertainty is            dwarfed by that of the numerator on the order of             part in  or even  in  compared to  in  for            the numerator and so can be ignored in the            calculation Even setting aside the downward bias the            precision implied by  percent  that we are fairly            confident that we know the true value to about  part            in   is unwarranted            After further contemplation and examination of            validation data the researchers decide that their best            estimate is that the raw estimate is low by between             and  percent of the estimated value uniformly            distributed The process by which they came to this            conclusion  possibly involving a series of            calculations based on the quality of monitoring test            sensitivity etc  is beyond the present scope One            might dispute the implicit claim that there is no            chance of false positives but it should be remembered            that uncertainty distributions are never going to be            perfect or beyond criticism The researchers are simply            of the opinion that the number of false positives will            with extremely high probability be exceeded by the            undercount            Rather than bundling these two sources of error            into a single distribution using intuition that may or            may not be right the researchers might have been            better off reading ahead and using one of the methods            for combining multiple sources of uncertainty Had they            done so they might well have concluded that the            misclassification error was indeed dwarfed by the            underreporting and returned to a single            quantification            The result of the researchers uncertainty            distribution is a uniform distribution for the annual            disease incidence over the range  How            should this be reported One option is to report             which conveniently is the rounded result for the            entire range It accurately implies that our precision            with a known error of up to  on either side of the            mean warrants only about one significant figure To            provide more precision the certainty interval            containing  or  of the probability mass could be            reported Again without implying too much precision            for the boundaries An uncertainty distribution should            itself not be stated in an overlyprecise manner It            is usually not a good idea to report the extremes and            imply that the corrected value certainly falls between            them Extreme values can be misleading to the reader            They are also very sensitive to the exact input            distributions used such as in the current example            where the input distributions with zero probability            beyond some range are good estimates for most of the            probability mass but they exclude extreme values that            the researchers do not actually believe have zero            probability                                    The choice and nature of subjective uncertainty          distributions          A detailed assessment of what needs to be considered          in developing uncertainty distributions for inputs in          this kind of analysis is beyond the present scope but it          is worth making a few comments to provide perspective and          help researchers get started The uniform distribution in          the preceding example provides the easiest teaching          example but is probably not realistic Even interpreting          it as an approximation it is unlikely that someone          believes some range of values are approximately equally          likely but a value slightly outside that range is          approximately impossible          Typically we have a point estimate and think the true          value of the parameter is likely near it and the          probability drops off as we deviate in either direction          This describes various distributions including the          normal logistic triangular where the probability          density is unimodal dropping off linearly to zero in          each direction forming a triangular density function          and others The choice among distribution shapes can be          made largely based on whether the researcher wants to          allow values to trail off to infinity or not and whether          the distribution is symmetrical A triangular          distribution while seldom appearing in nature might          effectively approximate someones beliefs and has the          advantage for pedagogic purposes of allowing calculations          using polynomial algebra Normal and logistic          distributions are easy to work with using numerical          methods          It turns out that the choice of the exact shape of the          distribution after the rough magnitude of uncertainty          has been determined is relatively unimportant Estimates          like those presented here are fairly stable across          unimodal distribution shapes as long as the probability          mass is substantially overlapping Ie if two          distributions have a very similar range for the middle           of their probability mass and also for the middle           they will have very similar implications in these          uncertainty calculations It should be remembered that          the purpose of these methods is to better represent          uncertainty and that goal is not well served by claiming          too much precision about the details of the inputs and          calculations          The question of whether an input distribution          corresponds to something found in nature brings up a          complicated philosophyofstatistics question What          exactly are these probabilities To give an abbreviated          answer they are subjective probabilities that take into          consideration all of the researchers knowledge except          the point estimate for the parameter of interest they          have calculated in the above example that would be the          annual disease incidence and any prior beliefs about          what the true value of that parameter is The          subjectivity of this probability should not be seen as          surprising or regarded as a limitation All of scientific          inquiry from hypothesis generation to study design to          drawing conclusions is a highly subjective and          sociologic process Furthermore the alternative to          specifying such a distribution is to produce calculations          based on the assumption that there is zero uncertainty          which is either a subjective belief itself or more          likely is strongly believed to be wrong          The restriction that prior beliefs about the true          value should be excluded from the researchers generation          of the input probabilities is a subtlety that relates to          how we can interpret the results and how the resulting          uncertainty would relate to random error if it were          included in the calculations While it is not necessary          to delve deeply into Bayesian analysis to do the simple          calculations proposed in this paper or to generally          revise our thinking about how certain most results are          a formal interpretation of the quantified uncertainty of          a result is that it is a Bayesian posterior distribution          based on a prior distribution ie belief about the          distribution before the research in question that          assigns equal likelihood to all values in the relevant          range To understand the importance of the prior          distribution consider the possibility that one of the          researchers in the previous example was very confident          that the actual incidence of the disease was above  In          that case upon seeing the results of the study she          would not believe that the whole range was equally          likely but instead would think the upper end of it was          more likely because her new beliefs would be based on a          combination of the study result and what she knew          before          The implicit assumption that all possible values were          equally likely called a flat prior is problematic          because a flat distribution across all possible values is          never realistic The next step in improving these          calculations should be to relax that assumption However          the problems inherent in the implicit assumption of a          flat prior which makes the calculations much easier to          perform and to understand are reduced by a few factors          First the relevant range condition says that the prior          only needs to be flat across the range that contains most          of the probability mass resulting from the calculation          eg in the above example it would only have to be          flat across  This means that the worst          problem of a totally flat prior that unrealistically          extreme values have to be considered to be as likely as          realistic values is absent Furthermore the intuition          we as readers have learned from years of interpreting          point estimates and frequentist confidence intervals is          to roughly treat them as calculations based on flat          priors and then roughly incorporate them into whatever          actual prior belief we have That is if a study reports          an estimate of  or some interval around it and we were          previously quite sure that the true value was  or more          the new evidence might push our beliefs downward but it          is not going to replace them This is just as true if the          interval is a standard confidence interval based only on          random sampling error or an uncertainty quantification          and our practiced intuition will be useful until these          methods are advanced                          Increasing complexity                      Example continued            The researchers wish to extrapolate the frequency of            disease from the study community to estimate the total            cases for the entire state with a population of             A naive way to introduce quantified            uncertainty into the calculation would be to treat the            original study of  people as a random sample            from a population of  The result could be            quantified using the usual frequentist statistical            methods with the result misleadingly suggesting high            precision a  confidence interval of             But greater uncertainty is introduced by the            extrapolation of the results which introduces unknown            levels of nonstochastic error Perhaps the sample            community was studied because it was particularly            convenient because it was more urban or had a better            local health department and so is different from the            state average            The researchers do not know the specific amount of            bias for their estimate but they recognize that there            is likely some bias Their best estimate is that the            actual rate for the state is most likely within             of the sample community but it is plausible that the            extrapolation is off by as much as  To fit these            probabilities the researchers use a symmetrical            triangular distribution from  to  of the point            estimate ie zero probability density at             linearly increasing to the midpoint  and linearly            decreasing to zero at  They could have chosen a            normal distribution or various other similarlyshaped            distributions to represent these beliefs with similar            outcomes            This new source of error now combines with the            original underestimate to produce a probability density            for the total number of cases in the state The            additional uncertainty from random sampling error is            small and can be ignored as inconsequential            Alternatively the random sampling error could be            incorporated into the researchers subjective            probability Objectively determinable stochastic            processes can be brought into the uncertainty            calculation differently from subjective uncertainty            but this introduces complexity that is left for future            analyses The purpose of the present analysis is to            consider cases where sampling error is absent or is            insignificant compared to other sources of error            The density for a given final value x which            results from a calculation involving two uncertain            values is the integral across values of the two            functions that produce x In the present case this is            relatively simple to calculate The probability            distribution for the total number of cases in the state            is described by the continuous approximation                        where the definition of gt describes the            triangular distribution and hs the uniform            distribution and k is the scale factor to make fx a            probability density function The continuous            approximation is necessary not just for computational            convenience but because the form of the error            distributions was continuous Since our practical            interest is for ranges of values and not the exact            probability for a given value nothing is lost by this            approximation            For this distribution the middle  of the            probability mass falls in the range  to             A normal distribution with a mean of  and a standard            deviation of  would also have represented the            researchers beliefs about the bias from extrapolation            and would have yielded the same interval after            rounding for the middle  of the probability            mass            Solving this equation and thus figuring out            uncertainty intervals is easy But adding much more            complication makes it unwieldy A third layer of            multiplicative uncertainty would require a double            integral over a more complicated product and so on An            uncertain input that entered the equation other than by            multiplying would be more complicated still Indeed            simply using the normal distribution for the            uncertainty from the extrapolation would make this            calculation considerably more complicated The            implication is clear with more than a few simple            sources of uncertainty closedform analytic            calculation is not a practical method for quantifying            it                                    Estimating complex combinations of          uncertainty          Any calculation with a large number of inputs is          likely to resist closedform calculation of uncertainty          and intuitive statements about total uncertainty are          likely to be worthless Large in this case can mean          as few as three or four inputs if they all introduce          uncertainty However there are tools developed in          finance and engineering that can be used to calculate          uncertainty in such health research          To estimate the probability density for parameters of          interest given multiple uncertain input values we          propose using Monte Carlo random numberbased numerical          methods as follows           Probability distributions are specified for the          inputs as presented above           A random draw is made from each of those          distributions producing one set of possible true values          The calculation the same one used to generate the point          estimate is carried out for those values to produce a          possible final value of the estimate           Step  is iterated a large number of times          producing a new candidate value for each new set of          random draws           These values are recorded and can then be used to          calculate the probability of the true value being in a          particular interval or grouped into fixedwidth intervals          and represented by a histogram that approximates the          probability density function for the total          uncertainty          This approach takes a difficult problem and          approximates the answer by carrying out simple          calculations a large number of times Since these simple          calculations are the ones that had to be constructed to          get the point estimate in the first place a relatively          small amount of extra effort is required Monte Carlo          simulations of this sort are used extensively for similar          calculations of uncertainty in business and engineering          applications often under the rubric risk analysis          and so there is userfriendly offtheshelf software that          does these calculations Further background in these          applications is available at the time of writing from          the manufacturer of the software we used at          httpwwwcrystalballcomriskanalysisstarthtml          Extremely complicated Monte Carlo simulations are used to          model everything from nuclear explosions to biological          evolution but the tools needed for present purposes are          usable by any competent quantitative researcher          Monte Carlo uncertainty calculations have been          proposed for the errors in a typical epidemiologic study                  which are much more complicated than the          errors considered in the examples presented here Such          applications involve complicated interactions of random          error selection bias measurement error and other          sources of uncertainty that compound in mathematically          complicated ways For the straightforward adding and          multiplying used in the examples presented here these          calculations are simple to program and do not require          much computer time          Indeed the biggest challenge for quantifying          uncertainty in these calculations  quantifying the          various input uncertainties  is partially ameliorated by          the ease with which different values can be run to          produce sensitivity analyses While sensitivity analyses          cannot determine which values are better to use they can          point out which ones matter enough to warrant more          attention                          A Monte Carlobased uncertainty calculation          As an example of a calculation combining many sources          of uncertainty we use a simplified version of Mead et          als frequentlyquoted calculation of the incidence of          foodborne disease in the US    The present          example a highly simplified version of the second          authors master thesis    is intended primarily to          illustrate the method rather than explore the details of          food safety data Powell Ebel and Schlosser have also          conducted a MonteCarlobased analysis of the uncertainty          in some of Mead et als numbers    An indepth          analysis of the uncertainty for a particular foodborne          disease risk can be found in Vose et al              Mead et als calculation was based on literally          hundreds of input numbers including           the US rate of total gastrointestinal illness           foodborne illness case counts from passive          surveillance data reported voluntarily from clinics and          outbreak reports health department investigations           case counts from active surveillance attempts to          enumerate every case at five county or statelevel          sites           several estimates of how many unobserved cases are          represented by one observed case in order to extrapolate          from data to true values           estimates of the fraction of cases of each disease          that should be attributed to food           several other extrapolations          The need to understand and quantify uncertainty is          obvious when we observe that Mead et al despite the          clearly uncertain data and calculations emphasized in          the abstract and press releases a result to two          significant figures  million US cases per year with          no statement of uncertainty or a plausible range of          values Widely quoted in this form this number implies          that the experts are confident that the result is not           million or  million but is somewhere between  and           million After all they did not report  million          which would tend to imply less precision Note a          weakness of relying on significant figures alone If they          had reported  million it would not have been clear          whether they were rounding to the nearest   or           million This would only become clear if multiple          numbers all with the same rounding were reported The          body of the Mead et al paper actually contains an          estimate to ten significant figures Such overly precise          reporting could be justified in a paper if intended to          help the reader duplicate the calculations rather than          as a conclusory statement This is unlikely to be the          explanation in the present case since their calculation          is difficult to duplicate from the information given and          this kind of replication is not common in health          research          Monte Carlobased numerical methods allow us to          estimate the uncertainty for calculations as complicated          as Mead et als The complexity of their effort to use          existing highly incomplete data to estimate US          foodborne disease incidence is illustrated by the          spreadsheet we developed to duplicate their calculations          which includes over  numerical cells more than  of          which are inputs from outside information Even if we          believe that the analysis reflected sciences best          knowledge about these values we can be sure that such an          estimate is not accurate to better than  But what more          can we say about the range of possible values          To answer this we examined the various inputs and the          certainty of their sources and developed a model that          incorporated estimates for each source of uncertainty          For the current example we use a simplified version of          the calculation reducing the list of  different          diseases to the  that contributed the most to the total          plus an other category and simplifying some of the          multiplicative correction factors used in the original          The first example presented here uses conservative          uncertainty distributions that are relatively small and          meanpreserving ie they use the original Mead et al          point estimates as the distribution mean Even with this          optimistic level of uncertainty it is easy to see the          need to avoid implying too much precision          The calculation is summarized in Table  It starts          with incidence rates of several diseases that are          partially attributable to foodborne transmission Two of          the three major contributors are based on incomplete          samples from monitoring efforts They are multiplied by a          factor of  to estimate the total incidence The total          incidence of each disease is then multiplied by an          estimate of the portion of cases that are foodborne          These figures are summed across the diseases to get the          incidence of foodborne illnesses attributable to known          pathogens The total cases from unknown sources is then          calculated by estimating the total cases of          gastroenteritis in the US and subtracting the cases of          known origins To get the portion of these that are          attributable to foodborne pathogens Mead et al assumed          that the foodborne portion is the same as that for known          pathogens This result is added to the incidence from          known pathogens to get a total          Every one of these inputs introduces uncertainty To          reflect this we introduced the following distributions          Most of these distributions have some external basis          However this calculation should be seen primarily as a          demonstration of the methods for doing the analysis and a          rough estimate of the minimal uncertainty in Mead et          als number rather than a claim that these are the          right distributions For the total cases of each of the          three identified diseases the point estimate is replaced          by a normal distribution with a mean of the point          estimate and a standard deviation of  percent of the          point estimate For the  other pathogens which each          contributed a relatively small portion of the total we          simply used the point estimates because the uncertainty          for each is inconsequential Assuming their errors are          uncorrelated they tend to average out leaving a          relatively tight distribution of total uncertainty If          one believes that inaccuracies in the estimates of          incidence rates are correlated across diseases the          overall uncertainty would be greater For the          multiplicative factor used for two of those we used a          symmetrical triangular distribution centered on the          original  with a range of           For the percent of each disease attributable to food          we used the point estimates for two of the pathogens          because they were fairly close to  leaving little          room for error and appeared to be reasonably solid          estimates The remaining pathogen Norwalklike viruses          accounts for most of the total cases and the fraction          that are foodborne is highly uncertain and far from          either  or  leaving room for substantial variation          Not only does this percentage affect the estimated number          of foodborne cases of that disease but it dominates the          overall estimated percentage of gastroenteritis cases          attributed to food and thus the estimate for cases of          unknown etiology Given the large impact of uncertainty          in this input parameter we conduct a sensitivity          analysis for its impact below For the initial example          we modeled the uncertainty as a uniform distribution on           centered on the Mead et al point estimate of           percent          For total cases of gastroenteritis we used a normal          distribution with a mean of the original point estimate          and a standard deviation of  percent of the original          estimate To represent the uncertainty of the assumption          that the portion of gastroenteritis cases of unknown          origin attributable to foodborne pathogens is the same as          for cases of known origin we draw the portion of unknown          cases from a normal distribution around the portion of          known cases after it is calculated since it varies with          each iteration with a standard deviation of  percentage          points          This example was constructed and calculated using a          Microsoft Excel spreadsheet and the offtheshelf Monte          Carlo simulation package Crystal Ball Decisioneering          Inc Denver Colorado We ran half a million iterations          of the model producing the histogram in Figure that          approximates the probability density for the total number          of cases that results from these input uncertainties It          would overstate the quality of our estimates to interpret          this as providing precise probability estimates But the          rough cut at estimating the total uncertainty is very          informative Even with these relatively conservative          estimates of uncertainty chances are about half that the          real total is outside the range of  million to           million          The Microsoft Excel  spreadsheet used to run the          Crystal Ball simulation is available as a additional          fileto this paper Running the simulations directly from          the spreadsheet requires a copy of Crystal Ball  or a          later compatible version          Additional file                               Click here for file          What should we make of such a result That depends on          our goal for the estimate in the first place If the goal          is to get an estimate into the scientific literature for          others to use it is probably a good idea to report the          entire distribution along with sensitivity analyses and          let others use them as they will Researchers interested          in combining this with other estimates of the value in          question might want to look at how likely the other          estimates are according to this calculation and how          likely this estimate is according to other calculations          A sophisticated policy maker trying to figure out how          much attention to devote to this problem might want to          look at the probability that the total was greater than          some critical level say ten million or one hundred          million Indeed even a rough cut might be sufficient to          answer important policy questions eg we are pretty          sure that there are more than fifty million cases per          year which means this is a bigger problem than most          people think                          Sensitivity analysis          This method for quantifying uncertainty lends itself          easily to sensitivity analysis of the inputs The          quantification of uncertainty is itself is sometimes          called a sensitivity analysis but despite involving some          of the same math uncertainty quantification and          sensitivity analysis are fundamentally different A          sensitivity analysis asks questions like if we are          wrong about a certain input how much does our best          estimate change An uncertainty distribution does not          answer that question Rather the uncertainty          distribution           is that best estimate the best          estimate of the probabilities of possible true values          given our knowledge An uncertainty distribution does not          report deviations from the result of an analysis it           is the result of an analysis  A          sensitivity analysis can be done to see how much that          distribution changes if we change one of our inputs          For example compare a second estimate identical          except that the distribution of foodborne attribution for          Norwalklike viruses is  instead of the previous          values of  This is chosen primarily to be          illustrative and emphasize the effect of changing the          mean and variance of the range It turns out though          that Mead et al based their input of  on a single          rough estimate in the literature which was  so the          new mean of  is actually closer The result          represented in Figure  shows that the new mean value is          about  million and that half the probability mass is          now in the narrower range of  to  million The          substantial difference between this and our previous          distribution makes clear that one of the most useful          things that could be done to improve our estimate of the          total cases is to reduce our uncertainty about this key          input Furthermore it calls to mind the question of          whether even experts who see the estimate of  million          which appears usually presented as fact in almost          everything written about foodborne illness in the US          have any idea that it hinges so significantly on this          one rather arcane guesstimate about how much of the          illness caused by one pathogen is attributable to          foodborne transmission                            Summary        It is possible to quantify uncertainty in complex        calculations in health research as is commonly done for        nonsamplebased calculations in business or engineering        In addition to simply being a more accurate presentation of        scientific knowledge such quantification could        dramatically increase the value of the underlying estimates        in several ways It would clarify whether the estimates are        certain enough for the purposes for which they are used        Furthermore it would suggest how likely further research        is to produce a substantially different answer and would        direct such research toward improving the particular inputs        that create more of the uncertainty The notion of        reporting uncertainty sometimes provokes opposition as if        the revelation of uncertainty were responsible for creating        the uncertainty But quantification does not introduce        uncertainty that did not previously exist but rather        replaces ignorance about the magnitude of that uncertainty        with the best available knowledge                    Competing interests        Phillips and a few of his students have received free        shortterm licences for the Crystal Ball software though        this analysis was carried out using previously purchased        copies After this paper was written and reviewed but        before resubmission Phillips was briefly retained as an        expert witness by The Delaco Company in litigation related        to the phenylpropanolamine study that is briefly        mentioned                    Authors contributions        CVP conceived of the goal developed the method of        analysis created the simple examples in the manuscript        and simplified the final example for the manuscript LMP        carried out the research and original analysis of the        indepth example foodborne illness and created the        programming for the calculation CVP composed the        manuscript LMP read and approved the final manuscript            