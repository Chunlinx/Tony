                    Background                  Normalization by controls identified a          priori           Assume that some genes will not change under the          treatment under investigation           Identify these core genes in advance of the          experiment housekeeping genes extrinsic controls           Normalize all genes against these genes assuming          they do not change           Done                          Normalization by selfconsistency           Assume that some genes will not change under the          treatment under investigation           Initially designate all genes as core genes           Normalize provisionally all genes against the          core genes under the assumption that the true abundance          of the core genes does not change           Determine which genes appear to remain unchanged          under this normalization make this set the new core           If the new core differs from the previous core          then go to step            Else done                          Modeling and estimation                      The basic model            Let             Y                           ijk               log             I                           ijk              denote the logarithm of the measured intensity of            the             k th spot in the             j th replicate assay of the             i th treatment group Thus             k ranges from  to             G the number of genes per array                        j ranges from  to             r                           i               the number of replicate arrays within the             i th treatment group and             i takes values from  to the            number of treatment groups The examples in this paper            use two treatment groups The logarithmic            transformation converts a multiplicative normalization            constant to an additive normalization constant We also            find that this transformation renders the error            variances more homogeneous than they are in the            untransformed data Then the error model corresponding            to Equation  is                        Y                           ijk                                          ij                                          k                                          ik                                                        ijk                           where the                           ij               log             N                           ij              are now the normalization constants                           k                                         ik               log             A                           ik              are the mean log relative abundance and the            differential treatment effects respectively and              is the error standard deviation            The treatment effects are the quantities of most            direct interest for comparing expression profiles We            assume that the residuals                           ijk              are independent and identically distributed and            have zero mean and unit variance For the significance            tests below we will further assume that the errors are            normally distributed                                Estimation by selfconsistency            Estimation of the parameters in Equation  is            carried out in an iteratively reweighted leastsquares            IRLS procedures First let             c                           k              indicate the assignment of the             k th gene to the core set             c                           k                if gene             k is not in the core and             c                           k                            G if gene             k is in the core where             G is the number of genes in the            core The vector             c is thus normalized                            k                          c                           k                These indicators play the role of weights in            an IRLS Although they do depend on other estimated            parameters in each iteration the weights are treated            as constants depending only on parameter estimates            from the previous iteration            The notion of selfconsistency arises in the            combined processes of identifying the core and            normalizing the data the choice of genes belonging to            the core depends on the normalization and the optimal            normalization depends on which genes are identified            with the core            We start by minimizing the core sum of squares SS                           C                                      over and  Note that one can add a constant to and            subtract the same constant from without changing SS                           C               This invariance corresponds to our inability to            estimate absolute abundances but relative abundances            only We therefore enforce an identifiability            constraint                            k                                        k                The minimization gives                        where             a and             n are the estimators for and             respectively overbars indicate averages over the            dotted subscripts for example             The normalized and scaled data are now given by                        Note that if all of the genes are placed in the            core we have                        as expected            Now we estimate the differential treatment effects            by minimizing the residual sum of squares                        of the normalized data over yielding                        Note that the matrix             d of differential treatment            effects obeys                            i                          r                           i                          d                           ik                as we would hope            Selfconsistency requires that the vector of core            indicators             c depend on the estimated            differential treatment effects             d  We have tried several methods            for implementing an appropriate dependence and find            that one of the simplest schemes works very well We            simply fix the proportion of genes in the core rank            the genes by the square of the estimated differential            treatment effect and remove from the core for the next            iteration those genes in the   quantile                        where is a threshold chosen to ensure that a fixed            proportion of genes are designated core genes Note            that a possible improvement to the algorithm might be            found by appropriate optimization of rather than simply            fixing it in advance            We carry out the estimation iteratively We start            with             c                           k                            G for all             k all genes in the core and            estimate                           ik              by Equation  We then update             c according to Equation  and            repeat the estimation of with this new             c  We stop when             c does not change from one            iteration to the next                                The local regression model            What we find in the analysis of experimental data            however is that Equation  with             N constant is not adequately            realistic A more flexible approach that covers the            contingencies of Equations  and many others is to            generalize Equation  to                        Y                           ijk                                          ij                                         k                                           k                                          ik                                          k                                         ijk                            where                           ij              is the normalization function now assumed            explicitly to depend on the mean log abundance                           k               The function  which scales the error variance            describes the heteroscedasticity or nonconstancy of            the variance which we here assume depends only on the            mean log intensity level The two functions are            constrained to vary slowly and thus can be estimated by            local regression            If Equation  is used to estimate the normalization            the departures from linearity manifest themselves as            systematic bias in the residuals Figure  In all the            data we have examined the resulting biases are small            and slowlyvarying function of the mean log intensity            and so can be estimated using local regression on             a the estimator for the mean log            abundance It should be noted that an additive            component of the variability with nonzero expectation            in addition to the multiplicative noise Equation             can when the logarithmic transformation is applied            lead to such nonlinear response curves Our approach            here is to develop a method flexible enough to allow            for all sources of nonlinearity including additive            noise We demonstrate the validity of this method for            these formally misspecified models in our simulation            studies below            Estimation of both the normalization function  and            of the heteroscedasticity is carried out by local            regression                                Local regression            Local regression is a generalization of the            intuitive idea of smoothing by using a moving average            In local regression one goes beyond computing the            local average of a set of measured points by            estimating at each value of the predictor variables            all of the coefficients in a             P thorder regression in which            the regression coefficients themselves are slowly            varying functions of the predictor variable            Computation of a moving average is thus a zeroth order            local regression The availability of inexpensive            powerful computing has sparked renewed interest in            local regression techniques and its theoretical            underpinnings have been extensively elucidated                           Modeling a response function as a function of a            predictor             u via local regression proceeds            in two steps First we estimate a function of two            variables             u and             u                         f              u               u                             u                            u              u              u                               P                           u              u              u              P               For fixed             u f              u               u  is a polynomial in             u with coefficients                           i                           u  These coefficients will be            constrained to vary slowly with             u  the quantitative rates of            change specified by a parameter introduced below            Second we estimate              u  as                        where             b is the vector of estimators for             In other words we estimate the coefficients and            evaluate the function at             u              u  The terms of order greater            than  vanish but the estimates for the remaining            zerothorder terms depend nevertheless on the estimated            higherorder coefficients as follows Given a dataset            consisting of             n pairs              u                           i                           v                           i                           i               n  we estimate the coefficients            at a point             u not necessarily corresponding            to any             u                           i              in the dataset by minimizing a weighted            sumofsquares over                         The weighting functions             w are given by                        where             W is a symmetric function having            a simple maximum at the origin strictly decreasing on             and vanishing for             u   For our application in            this paper we use the efficiently computed tricube            function                        The function             h is known as the bandwidth and            controls just how slowly             f varies with             u  We choose the bandwidth so as            to give equal span at all points             u  The span is defined as the            proportion of points             u                           i              contained in a ball of radius             h              u  This choice of bandwidth            function is used in Loess regression   For all of            the computations in this paper we have used a span of                        Minimization of Equation  over the coefficient            vector              u  results in linear equations            of the form                        b                           i                           u               L                           i                           u              v              Where             L                           i              is the linear operator appropriate to the             i th coefficient and             v is the vector with components                           k               Note that the             L                           i              depends on the order             P of the local regression For            any given value of             P the             L                           i              can be explicitly written down but quickly become            algebraically complicated            The local regression estimate of             f              u               u  is                        Because of this linearity the sampling            distributions for these coefficients are known and we            can compute their sampling variances in a            straightforward manner              To adapt this method to the problem of            normalization and simultaneously to implement            selfconsistency we take for the weighting functions            the product of a tricube and a core indicator                        where             c                           k              is the core indicator as given in Equation  and            the             a                           k              are given by Equation  In these terms the local            regression estimate             n of             v is given by                        with the normalized data given by                        and the differential treatment effects by                        Again we have                            i                          r                           i                          d                           ik                The core indicator vector             c is then iterated to fixation as            described in the previous section but with                            i                          r                           i                          d                           ik              compared against             s                            k               where             s   is the estimated local            variance discussed in the next section                                Local variance estimation            In addition to local nonlinearities in the response            curve we also find that the data are heteroscedastic            the error variance shows a clear dependence on the            estimated abundance The logarithmic transformation            removes a substantial part of this dependence but does            not flatten it out entirely One might try an             a priori accounting of the            sources of error and thereby provide a parametric model            for it but the number of potential error sources is            large so we instead choose a flexible error model and            estimate local variance by again using local            regression The technique involves computing the local            likelihood and the effective residual degrees of            freedom and is described in detail in   Their            ratio of the local likelihood and the effective degrees            of freedom provides a smooth estimate of the local            variance The estimated residuals are not strictly            linear functions of             Y because of the implicit            dependence of the indicator vector             c on the data             Y and because of our use of the            estimator             a rather than a strictly            independent variable as the predictor for the local            regression We expect these corrections due to            nonlinearities to be small and thus neglect them in our            estimates of the local variance            At this stage we have computed a firstorder            approximate solution for the estimation problem We may            now perform another iteration in addition to the            iterated solution for the core indicator             c  to improve the approximation            reweighting the data by the inverse of the estimated            local variance Our experience however has been that            the firstorder corrections are sufficient and the            higherorder corrections are more difficult to compute            and make little difference in the final analysis For            the applications and validation tests that follow we            use just the firstorder corrections                                Pairwise expressionlevel comparisons            We perform individual pairwise hypothesis tests for            each spot in the array by computing the statistic                        where             s              a                           k               is the squareroot of the local variance at the            mean relative expression value             a                           k               We test             z as a standard normal under the            null hypothesis of no expression difference                                    Validation          We illustrate the use of the computational methods by          fixing   and applying them to data generated in an          experiment carried out on cultured spontaneously          immortalized rat peritoneal mesothelial cells to          determine the transcriptional effects of treatment with          potassium bromate The data consist of measured          intensities of           G   genes from each of four          arrays two replicates           r                       r              in each of two treatment groups          A complete discussion of the biological results obtained          in these experiments can be found in                              Results and discussion                  Confirmation by quantitative PCR          Quantitative PCR analysis confirmed nine gene changes          The tenth           PLA could not be confirmed          because of lack of signal in both treatment groups and          was therefore likely to be due to a problem in the PCR          for that gene            Morphologic analysis revealed complete mitotic arrest          by  hours postexposure with increased numbers of          condensed cells with pyknotic nuclei believed to be          apoptotic Strong HOspecific staining was observed in          treated cells whereas control cells showed weak          nonspecific staining or no staining at all                          Statistical characteristics of the data          A histogram of mean log spot intensities Figure           shows that nearly a quarter of the  spots on the array          show little or no signal The remainder of the          distribution shows a very gradual maximum followed by a          long tail skewing the distribution to the right The          total range is about  natural logs corresponding to          approximately fold change from highest intensity to          background          The estimated variance of the log intensities          increases from the lowest log intensities for about one          natural log to peak at a value of about  and then          decreases to asymptote at about  for intense spots          This suggests that the error is dominated by different          sources in the two intensity regimes Furthermore the          fact that the variance of the log intensity decreases for          large intensities indicates that the variance scales like                    q  where           q             q   corresponds to lognormal          behavior with constant coefficient of variation and           q   corresponds to the          Poissonlike behavior of independent counting          processes          The four arrays in this study also showed          nonnegligible bias Figure  The rootmeansquare          RMS bias over all four arrays was     This          should be compared to the estimated standard deviation of          the residuals after bias removal of     it is          clearly comparable This bias is not likely not to be an          artifact of the fitting procedure Application of the          fitting procedure to simulated data without bias see          below results in a range of RMS bias that is much          smaller than that seen in the real data Tables                      In addition to the experiments reported here we have          examined data from several other microarray platforms and          find that in terms of the heteroscedasticity and apparent          bias they are qualitatively similar not shown                          Simulation studies          To determine the reliability of our methods we          generated simulated data under a number of models based          on the statistical characteristics of the data obtained          in our hybridization experiments All of the simulated          data was produced using FORTRAN programs calling IMSL          subroutines for sorting cubic spline interpolation and          random number generation          For each model and each set of conditions we ran           independent realizations The data from each of these          realizations was used as input to our normalization          routines which performed normalizations in two ways          First we normalized according to Equations   and           that is without bias removal and without accounting for          heteroscedasticity this procedure is referred to as          naive Then we normalized according to Equations            and  and   with bias removal and estimation of          heteroscedasticity The software that implements the          latter method is referred to as NoSeCoLoR for          Normalization by SelfConsistency and Local Regression            For judging the relative performance of the two          methods we recorded the number of true positives and the          number of false positives for each simulated dataset                          Homoscedastic error model          In the first set of tests the data were generated by          simulations of the model                    where the values for were generated as normals with          mean  and standard deviation  the                       k            were taken to be the values           a                       k            estimated from the experimental data   this          is the value estimated from the experimental data          treated as homoscedastic and                       ijk            were generated as standard normal The treatment          effects were generated by choosing at random a fixed          number of genes           G  or  of the total number           G  and within this set letting                      k                         k            log           f and                      k    Outside this set                       ik              Here                       k            are independently drawn from  with equal          probability and           f is the fold change or ratio of          expression level between treated and control groups          The function                       ij              representing nonlinearity and bias was taken to          be proportional to the corresponding function           n                       ij            estimated in the above data analysis Figure  and          completed by cubic spline interpolation The constant of          proportionality designated           q in the tables regulates the size          of the bias          What we find Table  is that the power of the test          for the naive analysis is diminished by the presence of          bias For the localregression analysis NoSeCoLoR the          power is unaffected by the presence of bias Furthermore          when the proportion  of affected genes among all genes          is small    the power of the two methods is about          the same When   the naive method has slightly          better power when bias is absent                          Heteroscedastic error model          As discussed above even the logtransformed data are          not homoscedastic but have variance that varies with the          mean intensity level The second set of simulations is          similar to the first but differs in that the constant            in Equation  is replaced by the          function   estimated from the Clontech array          experiments Figure  All other details are as for the          previous simulation          In this case Table  we find as before that bias          diminishes the power of the naive procedure but not that          of NoSeCoLoR In addition the rate of false positives is          now notably high for the naive method NoSeCoLoR yields          consistently smaller falsepositive rates although when          large proportions of genes are affected and have large          effect size the rate of false positives with NoSeCoLoR          is also larger than nominal                          Compound error model          The model given by Equation  is intended to be          flexible and to be a reasonable approximation to a          variety of models One particularly common source of          nonlinearity is additive error on the untransformed          data or background with nonzero mean Equation  We          have therefore simulated data according to a model given          by                    I                       ijk             exp                        k                       v                       ij                                    ik                                    ijk              exp                        ij                                    ijk                        where the terms   and have the meanings assigned          above and are computed as in the first simulation In          particular  has zero mean and constant variance with            The second exponential represents an additive          background This background is modeled as lognormal The          component                       ij            common to all spots in an array is chosen as a          normal random deviate with mean zero and standard          deviation           q Differences in from one array to          the other can create apparent biases in the          logtransformed data Figure  The genespecific term          in the background                       ijk            has mean zero and standard deviation           It is in this simulation that the naive method fails          most dramatically For all datasets the naive method          gives falsepositive rates significantly greater than          nominal some as much as tenfold higher than nominal          NoSeCoLoR has much better error rates although as seen          before performance starts to suffer when larger numbers          of spots are affected The power of comparisons using          NoSeCoLoR is again much more resistant to changes in the          effective bias level            c in Table  than is the naive          method                            Conclusions        We have presented a method for normalizing microarray        data that relies on the statistical consistency of relative        expression levels among a core set of genes that is not        identified in advance but inferred from the data itself        The normalization and variance estimation are both        performed using local regression We are then able to        perform standard comparison tests This technique reveals        biologically plausible expressionlevel differences between        control mesotheliomas and mesotheliomas treated with a        potent inducer of oxidative stress The expression changes        identified by our normalization methodology were confirmed        by quantitative PCR in all cases but one where there was no        detectable PCR amplification at all        Our simulation studies show that our normalization        technique performs well The worst case occurs when the        response curve is perfectly linear the variance constant        and a large proportion of genes experiences sizable        expressionlevel changes Under these conditions our        method has a falsepositive rate somewhat greater than        nominal and selfconsistent normalization without local        regression performs slightly better than that with local        regression On the other hand our method is insensitive to        bias and heteroscedasticity both of which have a        significant deleterious effect on the naive method        Furthermore bias and heteroscedasticity are both        measurably present in all data that we have examined from        microarrays from a number of different manufacturers and        from several different laboratories In these cases local        regression performs better than selfconsistency alone        When the data are generated by an        additiveplusmultiplicative error model the naive method        completely breaks down whereas our method continues to        perform well        We have applied these methods to the analysis of        microarray data in toxicogenomic studies    where        the results made good biological sense and where relevant        were confirmed by subsequent experimentation All        dataanalytic techniques benefit from extensive use and        assessment using several platforms and diverse biological        systems To facilitate this process for the methods        described here and to provide them to the interested        research community we have made the software used to        implement them available for noncommercial use          DNA hybridization microarrays promise unprecedented        insight into many areas of cell biology and statistical        methods will be essential for making sense of the vast        quantities of information contained in their data        Efficient and reliable normalization procedures are an        indispensable component of any statistical method further        development and analysis of error models for microarray        data will be a worthwhile investment                    Materials and methods                  Clontech microarrays          This is a brief description of the experimental          methods complete details can be found in            Immortalized rat peritoneal mesothelial cells FredPe          developed inhouse were grown in mesothelial cell culture          media as previously described   for several months          before experiment with weekly subculturing Cells plated          at    cells mm dish in  ml media were grown          for  h and treated with the predetermined ED            concentration of  mM KBrO            for  or  h Cells were detached          using a cell lifter and centrifuged at            g for  min The supernatant          medium was removed by aspiration and cells were          resuspended in  ml sterile PBS and frozen at C          until RNA extraction The Atlas Pure Total RNA protocol          for polyA mRNA extraction was used Samples were          hybridized in manufacturersupplied hybridization          solution Clontech ExpressHyb for  min at C After          hybridization the membranes were washed removed          wrapped in plastic wrap and placed against a rareearth          screen for  h followed by phosphoimager detection and          AtlasImage analysis before application of the software          tools described in this paper                          Quantitative PCR          Confirmation by Taqman PerkinElmer quantitative PCR          was performed for nine selected genes as described in            The genes selected for confirmation were those for          cyclin D GADD GPX HO HSP Mdr QR          prostaglandin H synthase PGHS pWAFCIP and PLA          Two control and two treated samples from the h time          point and two control and one treated from the h time          point were analyzed Each plate contained duplicate          wells of each gene and  notemplate control NTC          wells divided evenly among four quadrants                          Analysis          Software for the implementation of the statistical          estimation and testing procedures described above was          written in FORTRAN and run on desktop PCs            Additional statistical computations were performed using          Splus  MathSoft                            Additional data files        The additional data filesavailable or from   consist        of several files for implementing the methods described        here NoSeCoLoRexe is the executable file compiled for        Windows for the program itself NoSeCoLoRTheManualpdf        is the users guide and contains information on input        formatting and the interpretation of output files        READMEtxt contains instructions for installation and        startup there are several sample input files and        associated output files        Additional data file         A zip file containing several files for implementing the        methods described here        A zip file containing several files for implementing the        methods described here        Click here for additional data file            