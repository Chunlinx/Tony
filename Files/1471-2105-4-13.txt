                    Background        Alizadeh         et al     did a large scale        longterm study of diffuse large Bcell lymphoma DLBCL        using microarray data chips By doing cluster analysis on        this data they were able to diagnose  donors with an        accuracy of  for this specific lymphoma they were not        able to predict which individual patients would survive to        the end of the longterm study The International        Prognostic Index for this disease was incorrect for  of        these patients        Cluster analysis together with other statistical        methods for identifying and correlating minimal gene lists        with outcome have become established as the primary tools        for the analysis of microarray data in cancer studies We        wished to test a different approach ANN        These two approaches to the analysis of microarray data        differ substantially in their mode of operation In the        first examination of the data clustering as applied in        numerous recent cancer studies is an unsupervised mapping        of the input data examples based on the overall pairwise        similarity of those examples to each other here        similarity with respect to the expression levels of        thousands of genes the method is unsupervised in that no        information of the desired outcome is provided Subsequent        analysis of the clusters in these studies generally        attempts to reduce the gene set to the subset of genes that        are most informative for the problem at hand This step is        a supervised step since there is an explicit effort to find        correlations in the pattern of gene expression that match        the classification one is attempting to make among the        input examples see Discussion for specific examples The        input for this supervised step is the product of an        unsupervised step As this subselection is not routinely        subjected to independent test using input examples        originally withheld from the subselection process it is        generally not possible to judge how specifically the        subselection choices relate to this specific set of        examples as opposed to the general population of potential        examples To the extent that the gene set employed is much        larger than the gene set that really determines the        classification it is possible that much of the clustering        result will be based on irrelevant similarities        On the other hand backpropagation neural networks are a        supervised learning method that has an excellent reputation        for classification problems During the training phase the        ANN are supplied with both the input data and the answer        and are specifically tasked to make the classification of        interest given a training set of examples from all        classes That is the ANN are constantly checking to see if        they have gotten the correct answer the answer being the        actual classification not just the overall similarity of        inputs        Networks accomplish this by continually adjusting their        internal weighted connections to reduce the observed error        in matching input to output When the network has achieved        a solution that correctly identifies all training examples        the weights are fixed it is then tested on input examples        that were not part of the training set to see if the        solution is a general one It is only in this independent        test that the quality of the network is judged        Investigators are not limited to a single network It is        feasible to train a series of networks using say  of        the examples for training and holding back  for testing        A different   can be tested in a second network and so        on In this way with the training of ten networks each        input can be found in a test set one time and can        therefore be independently evaluated The data presented        below with the exception of a few cases are the output of        ten slightly different trained networks operating in test        mode which collectively evaluate the entire donor pool        This roundrobin procedure was employed in duplicate in        every trial described throughout this work The fact that        one ends up with  networks is not an impediment to        analysis since any future examples could be submitted to        all  networks for evaluation with a majority poll        deciding the classification That is six networks in        agreement on a particular input datum would determine the        classification of that input These networks are of        course likely to be very similar in that their training        sets differ only slightly        A second major advantage of backpropagation networks        follows from the first Not only are neural networks        trained to the specific question rather than a loose        derivative of that question and tested for generality but        they can also be asked for a quantitative assessment of how        they got the correct answer Numerical partial        differentiation of the network with respect to a given test        input example     allows one to see the networks        evaluation of the relative impact of each gene in arriving        at the correct answer for this particular input Cluster        analysis including the statistical correlations has no        corresponding highly focused sight for targeting specific        similarities as opposed to nonspecific similarities To        the extent that this is true neural networks should be        able to identify relatively small gene subsets which will        significantly outperform the initial gene sets in        classification and which will also significantly outperform        the gene subsets suggested by cluster analysis                    Results                  Determining patient prognosis from microarray          data          Cluster analysis     had shown that the  gene          expression panels for  DLBCL patients contained some          information relevant to the question of prognosis but          these authors did not make an attempt to provide survival          predictions for individual patients          We wished to see if the neural network strategy of          train test differentiate retrain on the reduced gene          set and retest could produce any useful result with          respect to prognosis on an individual basis The approach          would be    use the entire gene set without          preprocessing to train a network testing to confirm that          it had at least a good fit to the problem and    use          the networks definition of the problem by          differentiating the network to focus on those genes most          essential to the classification These genes would then          form the basis for training new networks with hopefully          improved performance Over  networks were trained for          this study Figure shows a work flow schematic for this          study Table provides a summary overview of the data          including data not shown          Initially a network was trained to accept microarray          data on the complete panel of  genes from           patients This network had  input neurons with a          semiquantitative assessment of each gene           middlelayer neurons and a single output neuron The          networks were originally designed with  input bits per          datum one for sign   and  for quantitative          degree of signal with  being  to   being           to   being  to  and  being           Thus  would indicate a particular gene whose          expression relative to control was increased at a          magnitude  The training set included  donors          with  additional donors being held back as test data          The network was trained by processing  iterations of          the complete training set The test set drawn from a          mixture of survivors and nonsurvivors was then run The          entire process was then repeated with a different choice          of test data each time In this roundrobin fashion all          donors serve as test data for one of the networks and          each training set is necessarily slightly different A          round robin series of  networks was generated Data          underlying Figure  of the earlier report          httpllmppnihgovlymphomadatashtmlwere used for          training The networks were asked to predict based on          the  gene set which of  DLBCL patients would          survive to the end of the study longest point            yrs Networks initially varied with from  to  errors          on  test patients each for a total of  of           patients correctly predicted data not shown However          a trained neural network can be numerically          differentiated     to show the relative dependence of          the output classification on each active input neuron          within an input vector Briefly stated the          differentiation process involves slightly perturbing the          activation down from  to  of each active input          neuron one at a time to note the specific change in the          output value In that there is one gene for each active          node the largest change in the output points to the most          influential gene We then trained qualitative networks          with  bits per gene on the  gene set in order to          differentiate them   for expression greater than or          equal to the control   for less than the control          The networks had  middle layer neurons This coding has          the effect that there is an active neuron for each gene          in the set regardless of expression level and the total          number of active input neurons is constant from input to          input By taking the top  of genes in each of           differentiations and requiring agreement of at least  of           patients in choosing each gene we obtained a set of           genes These cutoff criteria are necessarily          arbitrary and are only justified by subsequent proof that          they produced gene subsets having the desired          information A roundrobin series of  networks with           test donors each produced a single error DLCL in          survival predictions when trained on these  genes data          not shown  The second roundrobin training with the          same gene set produced no errors correctly evaluating          all  patients in a series of  test sets Table                    For a second study we took  patients and held them          in reserve to model information from a followup study          Twenty networks were trained on the  gene set using          the remaining  patients each had  patients in the          training set and  in the test set Collectively these          networks made no errors in the prognosis of  patients          The data for the  reserve patients were then tested on          all  trained networks to emulate followup data Out of           individual scores there were  errors distributed          over  patients A poll of the  networks therefore          produced no errors by a majority correctly classifying          all  members of the followup groupdata not          shown          The  genes are given in Table  In  of  cases          the gene chosen as most influential in determining the          correct prognosis was  a tyrosine kinase receptor          gene While this gene set may not be the absolute best          possible it clearly does contain sufficient information          for errorfree predictions on these patients The          identification of this gene set will hopefully lead          eventually to a better understanding of the interaction          of these genes in this disease as a result of future          studies                          Diagnosing lymphoma from microarray data          The diagnosis of DLBCL lymphoma by biopsy is not          trivial Even with gene expression data clustering          techniques produced a misreading of  out of  donors              a result unimproved in their hands by further          analysis of reduced gene panels We wished to see if back          propagation neural networks could do better using the          same data set Figure shows a work flow schematic for          this study This testing over the whole donor set with           genes produced  errors in diagnosis data not          shown          Thus in the first round ANN merely match cluster          analysis In preparation for differentiation a network          was trained with the same donor sets as the first network          above but coded qualitatively This network correctly          classified the  members of the test set data not          shown The  positive donors from the test set were each          used in turn to differentiate the network In these          cases the first criterion for selection was broad the          gene had to contribute at least  as much as the gene          making the maximum contribution to the correct          classification the second criterion was that  or more          of the donors had to agree on the selection This          produced a subset of  genes The number of genes          referenced by a given donor under identical criteria          ranged from  to  Only  of the genes overlapped          the  gene subset identified by cluster analysis It          was of interest to see if these genes were sufficient for          correct classification of the donors Ten different          networks were trained with the  gene subset Three          OCI Ly and DLBCL and tonsil errors were produced          over  donors in  separate series data not shown          At this point the neural networks were doing a          muchimproved diagnosis it remained to be seen if the          gene set could be further refined The set of  genes          was then treated in two different ways    it was          arbitrarily split into even and odd halves with each          half being used to train ten new networks    it was          used whole to train ten qualitative networks for further          differentiation          Twenty different networks were then trained using a           gene odd or even numbered subset of the  gene          set in  series of  The odd set again produced           errors data not shown In the even set a single error          was made over  donors in ten different test sets          identifying the tonsil inlier in the earlier cluster          analysis    as positive Table  Ten additional          networks were trained on the even set with the same          result data not shown          The differentiation of the networks from the  gene          set pointed to  genes Given the high accuracy of the          even  gene set we also trained networks on this set          for differentiation These pointed to  additional          genes In these cases only genes in the top  in          influence chosen in common by at least  of the          differentiated examples were considered Networks trained          on these  genes produced  errors over  donors in           test sets Table  The  genes using the designation          from the initial report are given in Table           We also wished to test this gene set in the context of          a followup study For this purpose we set aside           donors as followup data using the remaining  donors          in the usual trainingtesting round robin Eleven          networks were trained  with  training vectors and           test vectors and  with  training vectors and  test          vectors Collectively these produced  errors over           donors or  correct The followup donors were then          tested on the  networks A poll of these networks          showed a majority vote for  error or  correct                            Discussion        The rather remarkable conclusion of this analysis is        that there is sufficient information in a single gene        expression time point of less than  dozen genes to provide        perfect prognosis out to ten years and nearperfect        diagnosis for this set of donors Furthermore neural        networks through a strategy of train and differentiate        bring that information to the fore by progressively        focusing on the genes within the larger set which are most        responsible for the correct classifications providing at        once a reduction in the noise level and specific donor        profiles This focus on the specific classification problem        led to a set of  genes for prognosis and a second set of         genes for diagnosis These sets are mutually exclusive        The gene subsets suggested by cluster analysis    are        not supersets of these sets the  gene set of the        initial report captured only  of the  gene set used for        diagnosis and the  gene staging set captured only  of        the  gene set used for prognosis The  gene subset        proposed by Hastie         et al     for prognosis contains         of the  gene set There was no overlap with the  gene        set identified by Shipp         et al    to correlate with their        curedfatal classes for this disease At first it might        seem surprising that the gene subsets identified here do        not appear to be subsets of those identified earlier by        Alizadeh         et al  But this surprise is based on        a naive intuition The fact is that we do not know the        level of information redundancy that exists in these large        arrays Apropos of this point Alon         et al     discarded the         genes indicated by cluster analysis as most discriminatory        in their study of colon cancer and upon reclustering        found their diagnosis unimpaired Likewise it may be that        while the top  of relevant genes might be sufficient for        perfect classification so might the next  These sets        by definition are mutually exclusive By extension it is        not difficult to believe that some other large gene set        might be able to get  of the classifications correct        with little or no overlap with those genes in the top                We have been careful to avoid any claim that the gene        sets extracted in this procedure are the best gene sets        Only in one highly qualified sense can they be said to be        best that is in classifying         this data set there are no other        gene sets which offer a statistically significant        improvement in classification accuracy That is not to say        that there may not be other sets which could do as well        Nor is there any implication that these genes are seminal        in the etiology of this disease They may not be necessary        but they are sufficient to do this classification They may        not be sufficient to the classification of a much larger        patient set Forty patients are unlikely to be fully        representative of the general patient population with this        disease It should be noted however that the same caveats        apply to the analysis of these data by any other        method        There have been a number of additional studies of cancer        using microarray data for either prognostic or diagnostic        purposes The following listing includes a brief discussion        of  of these studies         Shipp         et al     did a study of  DLBCL        patients and  follicular lymphoma patients They first        sought to classify DLBCL and FL patients They clustered         genes Using their own weighted combination of        informative gene markers they picked out  genes whose        expression levels would be used to do a way        classification They correctly classified  patients        for a diagnostic accuracy of  They then attempted to        develop high risk and low risk groups with respect to         year prognosis They used several different methods for        associating particular gene clusters with survival outcome        Kaplan Meier analysis Support Vector Machine and        Knearest neighbor analysis They selected  genes as most        informative and achieved the best result with SVM modeling        They did not explicitly state how many patients initially        sorted into the high risklow risk groups but other data        suggest  and  respectively The only way in which these        survival probability plots can be compared to the patient        by patient predictions presented above is to associate low        risk with survival and high risk with nonsurvival Please        notethis equation was not made by any of the authors with        the exception of    below discussing risk groups If        one makes this association their best result is         errors for a  yr survival accuracy of          Rosenwald         et al     did what they termed a        followup study on the original Alizadeh         et al  study of DLBCL patients        However it was not really a followup study because a        different chip was used for the microarray data The        Alizadeh study identified  groups based on an analysis of        weighting the gene cluster groups germinal center B        celllike tumors which correlated with low risk and        activated B celllike tumors which correlated with high        risk If these groups were made survivors and        nonsurvivors the prognosis accuracy would have been         In the followup the authors found it necessary to        introduce a third group consisting of patients who did not        fit either of the previous  categories Although lacking        the associated gene profile this third group had a        survival pattern much like the activated B celllike group        The authors used Cox proportional hazards modeling to        assign groups on the basis of the expression of  genes        The  yr survival for the low risk group was   for        the activated B celllike group and  for the rd group        An improved result was obtained using  genes drawn from         signature gene groupings plus a score for BMP expression        Kaplan Meier estimates of survival were determined for         quartiles for which the  yr survival rate was         If these  are collapsed into  categories        of survivor and nonsurvivor it would produce         errors for a prognosis accuracy of          vant Veer         et al     did a study of         patients with breast cancer Starting with  signature        genes they narrowed down the gene pool to  genes by        examining the correlation coefficient of each gene with the        prognostic outcome They then rank ordered these genes and        added them  at a time to a onemanout test of their         patients for predicted outcome This was repeated until an        optimum outcome classification was reached This occurred        at  genes A patient by patient classification based on        the weighting of these  genes was able to produce a        survival classification with  errors for an accuracy        of          Beer         et al     used clustering and Cox        hazard analysis to generate a list of  genes to be used        in Kaplan Meier  yr projections of survival They had         patients with lung cancer in the study With  patients        originally assigned to the low risk group and  to the        high risk group the corresponding  yr survival rates        were  and  If treated as survival categories this        would produce  errors for a prognosis classification        accuracy of  Although these authors had complete  yr        survival data on  of the patients in the study they at        no point attempted to analyze this group specifically for        direct comparison with predictions         Khan         et al     used linear neural        networks to analyze microarray data from patients with        small round bluecell tumors They wished to classify the         subcategories of this tumor Principle Component Analysis        was used to reduce  genes to  components Neural        networks were trained using  of a  patient pool to        train and  to test in a fully crossvalidated fashion        The groups were shuffled  times to produce         networks These networks correctly classified all         patients in a way classification The networks were        analyzed for the most influential inputs to produce a list        of  genes New networks were calibrated with just these         genes these again correctly classified the  patients        and also correctly classified the  patients who had been        withheld from the whole process         Dehanasekaran         et al     did a study of         prostate biopsy samples  nontumorous tumor in situ         metastatic tumor Cluster analysis of microarray data        from nearly  genes misplaced  samples out of  for        a diagnostic accuracy of  The authors did not state why        they limited the clustering result to  samples when they        had  Although they performed additional analyses they        did not involve using the array data for either diagnosis        or prognosis         Golub         et al     wished to be able to        distinguish acute myeloid leukemia AML from acute        lymphoblastic leukemia ALL Starting with the expression        of  genes from  patients they did a class        clustering They then did a neighbor analysis to identify         genes occurring above chance levels which related to        the AMLALL distinction They choose an informative subset        of  genes to weight for class assignment of the patients        They were able to correctly classify  patients for a        diagnostic accuracy of  They next attempted to use        selforganizingmaps SOM for  classes in place of the        initial clustering This produced only  errors for         diagnostic accuracy Drawing a  gene predictor from these        SOM classes they again produced  errors maintaining a         accuracy These authors also attempted to use array        data to predict clinical outcome on  AML patients but        without success        The identification of specific genes associated with a        particular biological characteristic such as malignant        phenotype would be useful in many settings    Precise        classification and staging of tumors is critical for the        selection of the appropriate therapy At present        classification is accomplished by morphologic        immunohistochemical and limited biological analyses        Neural net analysis in the form of specific donor profiles        could provide a fine structure analysis of tumors        characterizing them by a precise weighting of the genes        which they express differentially At present only subsets        of patients with a given type of tumor respond to therapy        Networks trained to distinguish responders from        nonresponders would allow a comparison of tumorexpressed        genes in responders and nonresponders to find those genes        most predictive of response Recently we have used neural        networks on the data of Perou         et al     for classifying breast        tumors as hormonally responsive or nonresponsive Networks        that gave a perfect classification with  genes pointed        to a subset of  genes Retraining on these  genes        produced no error in classifying  tissue samples from        their study unpublished data We have also analyzed the        data of Dhanasekaran         et al     Here the original set        of  genes was reduced to  genes Retraining on these         genes gave no errors in a way normal early tumor        metastatic disease classification of  patients        unpublished data Given the significant impairment in the        quality of life for many patients undergoing chemotherapy        andor radiation therapy such prospective information        would be extremely beneficial    T cell and        antibodymediated immunotherapy may be efficacious        approaches for limiting tumor growth in cancer patients At        present there is a paucity of known tumor rejection        antigens that can be targeted Neural net analysis may        identify a panel of tumorencoded genes shared by many        patients with the same type of cancer and thereby provide a        repertoire of potentially novel tumor rejection antigens           For many patients with autoimmune disease the target        antigens is unknown Enhanced identification of celltype        specific markers of the target organ through neural net        profiling could identify potential target antigens as        candidate molecules for testing and tolerance        induction                    Conclusions        We believe neural networks will be an ideal tool to        assimilate the vast amount of information contained in        microarrays The artificial networks presented here were        not selected from a large number of attempts The networks        described here are the first or second attempts with the        data and format stated the longest training session lasted        less than  minutes Indeed the trained neural network        may in the form of its weight matrix have the best        possible understanding of the very broad statement being        made in the microarray a view that is accessible with the        differentiation of the network In this study that        viewpoint suggested a small subset of genes which proved        sufficient to give a nearperfect classification in each of        two problems This approach should be suitable for any        microarray study and indeed other global studies such as        D gels and massspec data which contain sufficient        information for training                    Methods        The data from microarray experiments are stored in        spreadsheet form representing the positive or negative        level of expression relative to some control state of        s of genes for two or more experimental conditions A        short software program is sufficient to translate these        data directly into a binary representation suitable as        input vectors for a neural network The neural network        software used throughout this study was NeuralWorks        Professional II Plus vNeural networks were trained on        the corresponding data sets with a fraction of the data        typically  withheld for testing purposes All open        fields in the data array were set to zero The trained        networks were then asked to classify new test data as to        donor type Since the gene expression levels are read        directly from the spreadsheet their order and names are        provided by the spreadsheet Given the large amount of        input data these networks generally converge to a low        error level very quickly during training often in a few        minutes or less Subsequently additional networks were        trained with a simplified input that contained only        qualitative information in the form of a plus or minus sign        to characterize the expression of each gene in the panel        This reduced the input size to  bits per gene  for        below the control and  for above or equal to the        control The output neuron was trained to output  for a        positive donor and  for a negative donor in the        diagnostic networks for the prognostic networks         indicated a nonsurvivor and  a survivor The  gene        panel network was provided respectively  or         middlelayer neurons for the  bit or  bit per gene        inputs With a very large number of input neurons it is        possible to overload the middlelayer neurons effectively        always operating them at one extreme limit or the other        this can have the undesirable effect of reducing their        sigmoid transfer function to a step function with the loss        of the networks nonlinearity This is clearly indicated        if multiple output values are found to be exactly        identical Networks were trained to an error level below         after which they were tested with previously unseen        data A possible disadvantage of neural networks        especially with a large input space and a relatively small        sample number is overtraining In overtraining a network        can learn the specifics of each training example as opposed        to finding a global solution for the entire training set        This behavior is characterized by a degradation in test        scores as training sessions are extended Although we saw        no evidence of this in this study we did look to see how        much additional training would be necessary to degrade the        test results in the case of the initial diagnosis networks        with  genes It was not until we doubled the training        iterations dictated by the  output error cutoff that we        saw some increased test error At double the normal        training interval  networks were unchanged but         networks showed an increased error of  This is        suggestive but not proof of the onset of overtraining        The networks trained on the reduced  or  gene sets had         or  middlelayer neurons        To differentiate a trained network with respect to        specific inputs a network was trained on the  gene        panel with  bits per gene The  positive donors from the        test set were each differentiated using software that we        designed for that purpose     The selected genes were        then compared among the  sets with genes occurring in         or more instances being included in the final subset This        requirement generated a subset of  genes from the        original  genes Networks were trained on this  gene        subset and on two  gene subsets representing every        other gene from the  set All were coded with  bits per        gene and employed networks with  or  middlelayer        neurons respectively Other networks were trained on the         gene set and the  even set coded with  bits per        gene for subsequent differentiation        The differentiation of the large panel networks trained        for prognosis arbitrarily employed more selective criteria        see text for subset determination with the result that a        single differentiation reduced the gene set from  genes        to  genes Subsequent networks demonstrated that this was        a highly effective selection        All networks in this study were threelayer back        propagation networks trained with a learning coefficient of         and a momentum coefficient of  using the generalized        delta learning rule and the standard sigmoidal transfer        function The cutoff in all cases between positive and        negative scoring was taken to be  RMS error at the        output neuron No network required more than  minutes        training time on a PC at  Mh in the majority of cases        the network was fully trained in less than a minute        Training and testing a  network roundrobin series could        generally be done in less than  minutes Training was        deliberately kept to a minimum to avoid overtraining The        networks represented here were in each case the first or        second attempt result for the given problem There was no        data trolling                    Note        All data not shown can be found at the site        httpresearchumbcedumoneillGBMS            