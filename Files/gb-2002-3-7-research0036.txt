                    Background                  Cluster analysis          In classification one is concerned with assigning          objects to classes on the basis of measurements made on          these objects There are two main aspects to          classification discrimination and clustering or          supervised and unsupervised learning In unsupervised          learning also known as cluster analysis class discovery          and unsupervised pattern recognition the classes are          unknown           a priori and need to be discovered          from the data In contrast in supervised learning also          known as discriminant analysis class prediction and          supervised pattern recognition the classes are          predefined and the task is to understand the basis for          the classification from a set of labeled objects          training or learning set This information is then used          to classify future observations The present article          focuses on the unsupervised problem that is on cluster          analysis but draws on notions from supervised learning          to address the problem          In cluster analysis the data are assumed to be          sampled from a mixture distribution with           K components corresponding to the           K clusters to be recovered Let            X                        X                       p             denote a random             p vector of explanatory variables          or features and let           Y             K  denote the unknown component or          cluster label Given a sample of           X values the goal is to estimate          the number of clusters           K and to estimate for each          observation its cluster label           Y           Suppose we have data           X             x                       ij             on           p explanatory variables for          example genes for           n observations for example tumor          mRNA samples where           x                       ij            denotes the realization of variable           X                       j            for observation           i and           x                       i                        x                     i  x                       ip             denotes the data vector for observation           i            i             n            j             p  We consider clustering          procedures that partition the learning set             x                       x                       n             into           K clusters of observations that are          similar to each other where           K is a userprespecified integer          More specifically the clustering   assigns class          labels            X                       i              to each observation where Clustering procedures          generally operate on a matrix of pairwise dissimilarities          or similarities between the observations to be          clustered such as the Euclidean or Manhattan distance          matrices   A partitioning of the learning set can be          produced directly by partitioning clustering methods for          example           k means partitioning around          medoid PAM selforganizing maps SOM or by          hierarchical clustering methods by cutting the          dendrogram to obtain           K branches or clusters Important          issues which will only be addressed briefly in this          article include the selection of observational units          the selection of variables for defining the groupings          the transformation and standardization of variables the          choice of a similarity or dissimilarity measure and the          choice of a clustering method   Our main concern here          is to estimate the number of clusters           K           When a clustering algorithm is applied to a set of          observations a partition of the data is returned whether          or not the data show a true clustering structure that          is whether or not           K   This fact causes no problems          if clustering is done to obtain a practical grouping of          the given set of objects as for organizational or          visualization purposes for example hierarchical          clustering for displaying large geneexpression data          matrices as in Eisen           et al   However if interest          lies primarily in the recognition of an unknown          classification of the data an artificial clustering is          not satisfactory and clusters resulting from the          algorithm must be investigated for their relevance and          reproducibility This task can be carried out by          descriptive and graphical exploratory methods or by          relying on probabilistic models and suitable statistical          significance tests for example             We argue here that validating the results of a          clustering procedure can be done effectively by focusing          on prediction accuracy Once new classes are identified          and class labels are assigned to the observations the          next step is often to build a classifier for predicting          the class of future observations The reproducibility or          predictability of cluster assignments becomes very          important in this context and therefore provides a          motivation for using ideas from supervised learning in an          unsupervised setting Resampling methods such as bagging            and boosting    have been applied          successfully in the field of supervised learning to          improve prediction accuracy We propose here a novel          resampling method Clest which combines ideas from          discriminant and cluster analysis for estimating the          number of clusters in a dataset Although the proposed          resampling methods are applicable to general clustering          problems and procedures particular attention is given to          the clustering of tumors on the basis of geneexpression          data using the partitioning around medoids PAM          procedure see below                          Partitioning around medoids          The new Clest procedure is demonstrated using the PAM          method of Kaufman and Rousseeuw   As implemented in          the cluster package in R and SPlus the PAM function          takes as its arguments a dissimilarity matrix for          example the Euclidean distance matrix as used here and a          prespecified number of clusters           K The PAM procedure is based on          the search for           K representative objects or          medoids among the observations to be clustered After          finding a set of           K medoids           K clusters are constructed by          assigning each observation to the nearest medoid The          goal is to find           K medoids that minimize the sum of          the dissimilarities of the observations to their closest          medoid The algorithm first looks for a good initial set          of medoids then finds a local minimum for the objective          function that is a solution such that there is no          single switch of an observation with a medoid that will          decrease the objective          The PAM method tends to be more robust and          computationally efficient than           k means In addition PAM provides          a graphical display the silhouette plot which can be          used to select the number of clusters and to assess how          well individual observations are clustered Let           a                       i            denote the average dissimilarity between           i and all other observations in the          cluster to which           i belongs For any other cluster           C  let           d            iC  denote the average          dissimilarity of           i to all objects of           C and let           b                       i            denote the smallest of these           d            iC  The silhouette width of          observation           i is           sil                       i                        b                       i                       a                       i            max           a                       i                       b                       i             and the overall average silhouette width is simply          the average of           sil                       i            over all observations           i                          i                      sil                       i                       n  Intuitively objects with large          silhouette width           sil                       i             are well clustered whereas those with small           sil                       i             tend to lie between clusters Kaufman and          Rousseeuw suggest estimating the number of clusters           K by that which gives the largest          average silhouette width                           Existing methods for estimating the number of          clusters in a dataset                      Null hypothesis            Suppose that the maximum possible number of clusters            in the data is set to             M                M              n  One approach to estimating            the number of clusters             K is to look for                 M that provides the strongest            significant evidence against the null hypothesis H              of             K   that is no clusters in            the data Two commonly used parametric null hypotheses            are the unimodality hypothesis and the uniformity            hypothesis            Under the unimodality hypothesis the data are            thought to be a random sample from a multivariate            normal distribution This model typically gives a high            probability of rejection of the null             K   if the data are sampled            from a distribution with a lower kurtosis than the            normal distribution such as the uniform distribution                         The uniformity hypothesis also referred to as            random position hypothesis states that the data are            sampled from a uniform distribution in             p dimensional space                Methods based on the uniformity hypothesis tend to            be conservative that is lead to few rejections of the            null hypothesis when the data are sampled from a            strongly unimodal distribution such as the normal            distribution In two or more dimensions and depending            on the test statistic the results can be very            sensitive to the region of support of the reference            distribution              For both types of hypotheses evidence against the            null hypothesis can be summarized formally under            probability models for the data or more informally by            using internal indices as described next                                Internal indices            Numerous methods have been proposed for testing the            null hypothesis             K   and estimating the number            of clusters in a dataset however none of them is            completely satisfactory Jain and Dubes   provide a            general overview of such methods The majority of            existing approaches do not attempt to formally test the            null hypothesis that             K   but rather look for the            clustering structure under which a summary statistic of            interest is optimal being large or small depending on            the statistic     These statistics are            typically functions of the withinclusters and            possibly betweenclusters sums of squares They are            referred to as internal indices in the sense that they            are computed from the same observations that are used            to create the clustering Consequently the            distribution of these indices is intractable In            particular as clustering methods attempt to maximize            the separation between clusters the ordinary            significance tests such as analysis of variance             F tests are not valid for            testing differences between the clusters Milligan and            Cooper   conducted an extensive Monte Carlo            evaluation of  internal indices Other approaches            include modeling the data using Gaussian mixtures and            applying a Bayesian criterion to determine the number            of components in the mixture   A recent proposal            of Tibshirani             et al   called the gap            statistic method calibrates an internal index such as            the withinclusters sum of squares against its            expectation under a suitably defined null hypothesis            note that gap tests have been used in another context            in cluster analysis by Bock   to test the null            hypothesis of a homogeneous population against the            alternative of heterogeneity Tibshirani             et al carried out a comparative            Monte Carlo study of the gap statistic and several of            the internal indices that showed a better performance            in the study of Milligan and Cooper   These            internal indices and the gap statistic are described in            more detail below            For a given partition of the learning set into               k              M clusters define             B                           k              and             W                           k              to be the             p              p matrices of between and within             k clusters sums of squares and            crossproducts   Note that             B              is not defined The following six            internal indices are commonly used to estimate the            number of clusters in a dataset                        sil  Kaufman and Rousseeuw              suggest selecting the number of clusters             k   which gives the largest            average silhouette width                           k               Silhouette widths were defined above with the            clustering procedure PAM                        ch  Calinski and Harabasz              For each number of clusters             k   define the index                        where tr denotes the trace of a matrix that is the            sum of the diagonal entries The estimated number of            clusters is argmax                         k               ch                           k                                      kl  Krzanowski and Lai   For            each number of clusters             k   define the indices                        diff                           k                            k                p tr             W                         k               k              p tr             W                           k              and                        kl                           k                            diff                           k                           diff                         k              The estimated number of clusters is argmax                         k               kl                           k                                      hart  Hartigan   For each            number of clusters             k   define the index                        The estimated number of clusters is the smallest             k   such that             hart                           k                                       gap or             gapPC  Tibshirani             et al   This method            compares an observed internal index such as the            withinclusters sum of squares to its expectation            under a reference null distribution as follows For            each number of clusters             k   compute the            withinclusters sum of squares tr             W                           k               Generate             B here             B   reference datasets under            the null distribution and apply the clustering            algorithm to each calculating the withinclusters sums            of squares tr             W                           k                tr             W                           k                          B  Compute the estimated gap            statistic                        and the standard deviation             sd                           k              of log tr             W                           k                          b                b              B Let  The estimated number of            clusters is the smallest             k   such that  where             k  argmax                         k              gap                           k                          Tibshirani             et al   chose the uniformity            hypothesis to create a reference null distribution and            considered two approaches for constructing the region            of support of the distribution In the first approach            the sampling window for the             j th variable               j              p  is the range of the observed            values for that variable In the second approach            following Sarle   the variables are sampled from a            uniform distribution over a box aligned with the            principal components of the centered design matrix            that is the columns of             X are first set to have mean  and            the singular value decomposition of             X is computed The new design            matrix is then backtransformed to obtain a reference            dataset Whereas the first approach has the advantage            of simplicity the second takes into account the shape            of the data distribution Note that in both approaches            the variables are sampled independently The version of            the gap method that uses the original variables to            construct the region of support is referred to as gap            and the second version as gapPC where PC stands for            principal components            Note that of the above methods only hart gap and            gapPC allow the estimation of only one cluster in the            data that is                                  External indices            The term validation of a clustering procedure            usually refers to the ability of a given method to            recover the true clustering structure in a dataset            There have been several attempts to assess validity on            theoretical grounds    however such approaches            turn out to be of little applicability in the context            of highdimensional complex datasets In many            validation studies clustering methods are evaluated on            their performance on empirical datasets with             a priori known cluster labels              or more commonly on simulation studies where true            cluster labels are known To assess the ability of a            clustering procedure to recover true cluster labels it            is necessary to define a measure of agreement between            two partitions the first partition being the             a priori known clustering            structure of the data and the second partition            resulting from the clustering procedure In the            clustering literature measures of agreement between            partitions are referred to as external indices several            such indices are reviewed next            Consider two partitions of             n objects             x                            x                           n               the             R class partition               u                            u                           R               and the             C class partition               v                            v                           C               External indices of partitional agreement can            be expressed in terms of a contingency table Table             with entry             n                           ij              denoting the number of objects that are both in            clusters             u                           i              and             v                           j                           i               R              j               C   Let and denote the row            and column sums of the contingency table respectively            and let             The following indices can then be used                         Rand  Rand                                       Jaccard  Jain and Dubes                                       FM  Fowlkes and Mallows                          Note that Rand and FM are linear functions of             Z  and hence are linear            functions of one another conditional on the row and            column sums in Table  If the row and column sums in            Table are fixed but the partitions are selected at            random that is if there is independence in the table            the hypergeometric distribution can be applied to            determine the expected value of quantities such as             Z  In particular                        An external index S is often standardized in such a            way that its expected value is  when the partitions            are selected at random and  when they match perfectly            This amounts to computing a standardized external            index                        where             S                           max              is the maximum value of the statistic             S and             E              S  is the expected value of             S when partitions are selected at            random Accordingly an often used correction for the            Rand statistic is                        The significance of an observed external index is            usually assessed under the assumption that the two            partitions to be compared are independent This            assumption does not hold for the resampling methods            described in the following section since the same data            are used to produce the two partitions Nevertheless            external indices are convenient tools for comparing two            clusterings and are used in the new resampling method            Clest In this context one should think of these            indices as internal rather than external measures                                      Results                  Clest a predictionbased resampling method for          estimating the number of clusters          We propose a new predictionbased resampling method          Clest for estimating the number of clusters if any in          a dataset The idea behind Clest is very intuitive if one          is concerned with reproducibility or predictability of          cluster assignments          It is proposed to estimate the number of clusters           K by repeatedly randomly dividing          the original dataset into two nonoverlapping sets a          learning set           b and a test set  For each          iteration and for each number of clusters           k a clustering            b  of the learning set           b is obtained and a predictor           C            b  is built using the class labels          from the clustering The predictor           C            b  is then applied to the test set          and the predicted labels are compared to those produced          by applying the clustering procedure to the test set          using one of the external indices or similarity          statistics described in the Background section The          number of clusters is estimated by comparing the observed          similarity statistic for each           k to its expected value under a          suitable null distribution with           K   The estimated number of          clusters is defined to be the corresponding to the          largest significant evidence against the null hypothesis          of           K            An early version of this approach was introduced by          Breckenridge   under the name of replication analysis          and was designed to evaluate the stability of a          clustering In the original replication analysis the          number of clusters           k is fixed and the data are          randomly divided into two samples A clustering procedure          partitions both samples into           k clusters and the centroids of          the clusters of the first sample are computed A second          set of labels is assigned to the observations in the          second sample by assigning to each observation the          cluster label of the closest centroid from the first          sample Finally an external index is used to assess the          agreement between the two partitions of the second          sample This measure reflects the stability of the          clustering structure The Clest procedure proposed here          generalizes the work of Breckenridge                            Clest procedure for estimating the number of          clusters in a dataset          Denote the maximum possible number of clusters by           M             M            n  For each number of clusters           k              k            M  perform steps            Repeat the following           B times          a Randomly split the original learning set into two          nonoverlapping sets a learning set           b and a test set           b Apply a clustering procedure to the learning set           b to obtain a partition            b           c Build a classifier           C            b  using the learning set           b and its cluster labels          d Apply the resulting classifier to the test set                    e Apply the clustering procedure to the test set to          obtain a partition            f Compute an external index           s                       kb            comparing the two sets of labels for  namely the          labels obtained by clustering and prediction           Let           t                       k             median           s                     k              s                       kB             denote the observed similarity statistic for the           k cluster partition of the          data           Generate           B            datasets under a suitable null          hypothesis For each reference dataset repeat the          procedure described in steps  and  above to obtain           B            similarity statistics           t                     k              t                     k            Bo             Let denote the average of these           B            statistics  and let           p                       k            denote the proportion of the           t                     k            b               b            B             that are at least as large as the          observed statistic           t                       k             that is the           p value for           t                       k             Finally let           d                       k                       t                       k             denote the difference between the observed          similarity statistic and its estimated expected value          under the null hypothesis of           K            Define the set           K as                    K              k            M            p                       k                       p                       max                       d                       k                       d                       min                      where           p                       max            and           d                       min            are preset thresholds see Parameters of the Clest          procedure section below If this set is empty estimate          the number of clusters as   Otherwise let  argmax                       k                                  K                       d                       k             that is take the number of clusters that          corresponds to the largest significant difference          statistic           d                       k                                      Parameters of the Clest procedure                      Clustering procedure partitioning around medoids            PAM            The PAM clustering procedure of Kaufman and            Rousseeuw   implemented in the cluster package in            R and SPlus was used to cluster observations based on            the Euclidean distance metric see Background                                Classifier diagonal linear discriminant analysis            DLDA            For multivariate Gaussian class conditional            densities that is for             x              y              k  N                           k                                         k               the maximum likelihood ML discriminant rule            or Bayes rule with uniform class priors predicts the            class of an observation             x by that which gives the largest            likelihood to             x  that is                        When the class densities have the same diagonal            covariance matrix  the discriminant rule is linear and            given by                        For the corresponding sample ML discriminant rules            the population mean vectors and covariance matrices are            estimated from a learning set by the sample mean            vectors and covariance matrices respectively and             For the constant covariance matrix case the pooled            estimate of the common covariance matrix is used             where             n                           k              denotes the number of observations in class             k and             n is the total sample size DLDA            is a very simple classifier but it has been shown to            perform well in complex situations in particular in            an extensive study of discrimination methods for the            classification of tumors using geneexpression data              DLDA is also known as naive Bayes            classification                                Reference null distribution            The reference datasets are generated under the            uniformity hypothesis as in the gap statistic method            see Background                                External index            All the external indices described in Background            were considered The FM index   was found to be            superior to the other indices when reference datasets            are generated under the uniformity hypothesis data not            shown                                Threshold parameters p maxand d min            The choices             p                           max                and             d                           min                are             ad hoc and can probably be            improved upon Nevertheless this rule gives a            satisfactory performance and is used in the absence of            a better choice                                Number of iterations and reference            datasets            Here we used             B              B                In general the Clest            procedure is robust to the choice of             B and             B              data not shown                                    Comparison of procedures on simulated data          The new procedure Clest was compared to six existing          methods presented in Background using data simulated from          the models described in Materials and methods Figure          displays bar plots for the percentage of simulations for          which a given method correctly recovered the number of          clusters for each of the eight models Table provides a          more detailed account of the simulation results for each          procedure It can be seen that Clest gave uniformly good          results over the range of models its worst performance          being for Model  with two overlapping clusters The rest          of the methods failed for at least one of the eight          models considered The gap procedure failed twice Models           and  and gapPC failed once Model  Neither gap nor          gapPC were able to identify the presence of the two          clusters for Model  which is a model with two drawnout          clusters and seven noise variables with varying          variances Both gap and gapPC consistently estimated one          cluster for this model perhaps because both methods are          based on the withinclusters sums of squares and          consequently are more affected by the variables with          larger variances In a majority of the simulations from          Model  Clest gap and gapPC failed to distinguish          between one and two clusters while the simple hart index          performed well The rest of the procedures do not have          by definition the ability to estimate one cluster and          hence generally identified the two clusters          Interestingly for Model  with three overlapping          clusters sil and ch performed poorly choosing two          clusters in a majority of the simulations while hart and          Clest showed the best performance Overall most methods          tended to underestimate more often than they          overestimated the number of clusters but the situation          was reversed for hart and kl For Model  it is only fair          to compare Clest gap gapPC and hart as the other          methods only estimate            In summary for the simulation models considered here          Clest was the most robust and accurate whereas hart          performed worst gapPC was better than gap and the rest          of the methods performed similarly          For a given model it is of interest to consider the          median value of the statistics used by each method to          estimate the number of clusters For each number of          clusters           k the plots of the median values          over the  simulated datasets of the Clest           d           k  statistic           gapPC           k   and                       k            statistics are shown in Figures  and           respectively The           d statistic does not generally          have local maxima except for Model  There a local          maximum appears at           K   clusters but the global          maximum occurs at           K   It can be seen that the          ability of Clest to distinguish between one and two          clusters is very low for Model  the median of the           d            values is less than the significance          cutoff           d                       min            used in the Clest procedure Indeed the results in          Table show that Clest identified two clusters for only           of the datasets simulated from Model  The figures          suggest that for the majority of the models the global          maximum of the median           d statistic is more pronounced          than the global maxima of the median           gapPC           k  and                       k            statistics respectively This again suggests good          robustness and accuracy properties for the Clest          method                          Comparison of procedures on microarray data          The new Clest method was also evaluated using          geneexpression data from the four cancer microarray          studies described in Materials and methods and summarized          in Table  Recall that mRNA samples in the lymphoma          leukemia and NCI datasets were assigned class labels          from the laboratory analyses of the tumor samples or from                    a priori knowledge of the cell          lines For the melanoma dataset tumor class labels were          obtained from the statistical analysis described in          Bittner           et al   In the discussion          that follows these class labels are treated as known          The six methods described in Background and Clest were          applied to estimate the number of clusters for each of          the four microarray datasets the results are presented          in Table           The methods Clest and sil correctly estimated the          presumed number of classes for all but the NCI dataset          where both methods identified three clusters only The          gap and gapPC methods overestimated the number of          clusters for all datasets with the exception of gapPC          which identified eight clusters for the NCI dataset          The ch method estimated two clusters for each of the four          datasets whereas kl and hart identified four classes for          the lymphoma dataset          For Clest gapPC and sil we further investigated how          the strength of the evidence for the estimated number of          clusters varied between datasets Figure displays plots          of the           d                       k                       gapPC                       k             and                       k            statistics versus the number of clusters           k Error bars for           d                       k            and           gapPC                       k            are based on the standard deviations of           t                       k            and log tr           W                       k            under their respective null distributions Whereas          the evidence for the existence of clusters is very strong          for the lymphoma leukemia and NCI datasets the          evidence for the two clusters in the melanoma dataset is          much weaker In particular for Clest the maximum value          of the           d                       k            statistic barely reaches the           d                       min            threshold of  For the leukemia dataset the           d                       k            statistic clearly peaks at           k   clusters and drops off          abruptly for the lymphoma and NCI datasets the          decrease is more gradual Note that according to Clest          there was not enough evidence to identify the two DLBCL          subclasses for the lymphoma dataset Alizadeh           et al   identified these          subclasses using subject matter knowledge to select the          genes for the clustering procedure here the genes were          selected in an unsupervised manner                            Discussion        Resampling methods such as bagging and boosting have        been applied successfully in a supervised learning context        to improve prediction accuracy Here and in a related        article   we have proposed resampling methods to        address two main problems in cluster analysis estimating        the number of clusters if any in a dataset improving and        assessing the accuracy of a given clustering procedure As        the groups obtained from cluster analysis are often used        later on for prediction purposes the approaches to these        two problems rely on and extend ideas from supervised        learning Although the methods are applicable to general        clustering problems and procedures particular attention is        given to the clustering of tumors using geneexpression        data The performance of the proposed and existing methods        was compared using simulated data and geneexpression data        from four recently published cancer microarray studies        To estimate the number of clusters in a dataset we        propose a predictionbased resampling method Clest which        estimates the number of clusters         K based on the reproducibility of        cluster assignments In comparative studies Clest was        generally found to be more accurate and robust than six        existing methods For the simulated datasets Clest        performed well across a wide range of models with varying        numbers of overlapping and nonoverlapping clusters        different numbers of variables and covariance matrix        structures Unlike methods based on between or        withinclusters sums of squares the resampling method        seems robust to the varying covariance structure of the        variables        For the microarray datasets Clest and sil correctly        estimated the number of tumor or cellline clusters as        determined from         a priori known or putative tumor and        cellline classes for three out of the four datasets the        performance of other methods was significantly worse We        focus here on the clustering of tumor mRNA samples using        geneexpression data Once tumor classes are specified an        important next step would be the identification of marker        genes that characterize these different tumor classes A        related question which we have not considered here is the        transpose clustering problem that is the clustering of        genes that have similar expression levels across biological        samples One could then investigate the clusters for the        presence of shared regulatory motifs among the genes          This could lead to the identification of genes that are not        only coexpressed but are also under similar regulatory        control Joint analysis of transcript level and sequence        data should lead to greater biological insight into the        molecular characterization of tumors        A number of decisions were made regarding the different        parameters of the Clest procedure The clustering procedure        PAM was used in the comparison however one should keep in        mind that different clustering procedures can generate        different partitions of the same data possibly leading to        different inferences about the number of clusters In        addition the clustering PAM and prediction methods DLDA        or naive Bayes considered in this article focus on similar        features of the data namely the distance of the        observations from cluster centers More work is needed to        investigate the robustness of Clest to these choices In        particular it would be interesting to consider prediction        and clustering methods that focus on different aspects of        the data for example classification trees instead of        DLDA Although it may seem that having a classifier as a        parameter of the Clest procedure creates more room for        error we have found that this is not the case in practice        When the classifier in Clest performs poorly other methods        for estimating the number of clusters also perform poorly        Another important choice in the Clest procedure is the        reference null distribution used to calibrate the observed        similarity statistics         t                   k          for different numbers of clusters The uniformity        hypothesis was used here a natural alternative would be to        consider random permutations of the variables that is        permutations of the entries of the design matrix within        columns In Clest the observed similarity statistics         t                   k          are compared across numbers of clusters         k by considering their distance from        their estimated expected value under the null distribution        A more sensitive calibration may be achieved by taking        scale into account that is by dividing the difference        statistic         d                   k          by the standard deviation of         t                   k          under the null distribution or even by considering         p values         p                   k          for         t                   k           We briefly considered these refinements and found        that on their own they did not allow good discrimination        between the different         k s The Clest method does however        use the idea of         p value in combination with the        differences         d                   k           as it imposes an upper limit on the         p value         p                   k           Finally the choice of cutoff parameters         d                   min          and         p                   max          was rather         ad hoc and could be fine tuned        We have not considered modelbased methods such as the        Bayesian approach of Fraley and Raftery   or the        mixturemodel approach of McLachlan         et al   Another issue only        briefly addressed here is the selection of variables on        which to base the clusterings For the microarray datasets        genes were selected on the basis of the variance of their        expression levels across samples and it was found that the        clusterings were fairly robust to the number of genes        Resampling methods are promising tools for addressing        various problems in cluster analysis BenHur         et al   have recently proposed a        stabilitybased method for estimating the number of        clusters where stability is characterized by the        distribution of pairwise similarities between clusterings        obtained from subsamples of the data It would be        interesting to relate the approach of BenHur         et al and Clest Elsewhere we        proposed two bagged clustering methods for improving and        assessing the accuracy of a given partitioning clustering        procedure   There the bootstrap is used to generate        and aggregate multiple clusterings and to assess the        confidence of cluster assignments for individual        observations Leisch   proposed a bagged clustering        method which is a combination of partitioning and        hierarchical methods A partitioning method is applied to        bootstrap learning sets and the resulting partitions are        combined by performing hierarchical clustering of the        cluster centers This method is similar in spirit to our        two new bagging procedures                      Conclusions        Focusing on prediction accuracy in conjunction with        resampling produces accurate and robust estimates of the        number of clusters As reproducibility of the cluster        assignments is an integral part of the Clest method the        clustering results can be used reliably for building a        classifier to predict the class of future observations In        addition the procedure is robust to the covariance        structure among variables                    Materials and methods                  Simulation models          Procedures for estimating the number of clusters in a          dataset were evaluated using simulated data from a          variety of models including those considered by          Tibshirani           et al   The models used for          comparison contain different numbers of overlapping and          nonoverlapping clusters different numbers of variables          and a wide range of covariance matrix structures In          addition a variable number of irrelevant or noise          variables are included in the models A noise variable is          a variable whose distribution does not depend on the          cluster label and such variables are added to obscure          the underlying clustering structure to be recovered                    Model  One cluster in           dimensions           n   observations are simulated          from the uniform distribution over the unit hypercube in           p   dimensions                    Model  Three clusters in two          dimensions The observations in each of the three          clusters are independent bivariate normal random          variables with means   and           respectively and identity covariance matrix There are            and  observations in each of the  clusters          respectively                    Model  Four clusters in           dimensions  noise variables Each cluster is randomly          chosen to have  or  observations and the          observations in a given cluster are independently drawn          from a multivariate normal distribution with identity          covariance matrix For each cluster the cluster means          for the first three variables are randomly chosen from a          N           o                       I             distribution where           o                       p            denotes a             p vector of zeros and           I                       p            denotes the           p            p identity matrix The means for          the remaining seven variables are  Any simulation where          the Euclidean distance between the two closest          observations belonging to different clusters is less than           is discarded                    Model  Four clusters in           dimensions Each cluster is randomly chosen to contain           or  observations with means randomly chosen as N           o                        I             The observations in a given          cluster are independently drawn from a normal          distribution with identity covariance matrix and          appropriate mean vector Any simulation where the          Euclidean distance between the two closest observations          belonging to different clusters is less than  is          discarded                    Model  Two elongated clusters in          three dimensions Cluster  contains  observations          generated as follows Set           x                       x                       x                       t with           t taking on equally spaced values          from  to  Gaussian noise with standard deviation          of  is then added to each variable Cluster  is          generated in the same way except that the value  is          added to each variable This results in two elongated          clusters stretching out along the main diagonal of a          threedimensional cube with  observations each                    Model  Two elongated clusters in           dimensions  noise variables The clusters are generated          as in Model  but in addition seven noise variables          are simulated independently from a normal distribution          with mean  and variance           v for the           v th variable             v                      Model   Two overlapping clusters in           dimensions  noise variables Each cluster contains           observations The first variable in each of the two          clusters is normally distributed with mean  and           respectively and with variance  The remaining nine          variables are simulated from the N           o                       I             distribution independently of the          first variable                    Model  Three overlapping clusters          in  dimensions  noise variables Each cluster          contains  observations The first three variables have          a multivariate normal distribution with mean vectors            and  respectively and          covariance matrix  where                        ij                          i   and                        ij                          i            j   The remaining  variables          are simulated independently from the N           o                       I             distribution          Note that Models    and  were considered in          Tibshirani           et al   Model  is similar to          the third model in   with the addition of seven          noise variables Model  is the same as Model  with the          addition of seven noise variables          Fifty datasets were simulated from each model and the          methods described in the Background and Results sections          were applied to estimate the number of clusters in the          resulting datasets We are primarily interested in          comparing the percentage of simulations for which each          procedure recovers the correct number of clusters as          this quantity reflects the accuracy of the procedure          However for the purpose of future applications it is          useful to also know whether a method tends to          underestimate or overestimate the true number of          clusters Hence the full distribution of the number of          clusters estimated by each method is presented in Table           Note that only the methods Clest gap gapPC and hart          have the capability to identify one cluster in the          data                          Microarray data                      Lymphoma            This dataset comes from a study of gene expression            in the three most prevalent adult lymphoid            malignancies Bcell chronic lymphocytic leukemia            BCLL follicular lymphoma FL and diffuse large            Bcell lymphoma DLBCL see    for a detailed            description of the experiments Geneexpression levels            were measured using a specialized cDNA microarray the            Lymphochip containing genes that are preferentially            expressed in lymphoid cells or which are of known            immunological or oncological importance In each            hybridization fluorescent cDNA targets were prepared            from a tumor mRNA sample redfluorescent dye Cy and            a reference mRNA sample derived from a pool of nine            different lymphoma cell lines greenfluorescent dye            Cy The cell lines in the common reference pool were            chosen to represent diverse expression patterns so            that most spots on the array would exhibit a nonzero            signal in the Cy channel This study produced            geneexpression data for             p   genes in             n   mRNA samples The tumor            mRNA samples consist of  cases of BCLL  cases of            FL and  cases of DLBCL Alizadeh             et al   further showed that            the DLBCL class is heterogeneous and comprises two            distinct subclasses of tumors with different clinical            behaviors The geneexpression data are summarized by            an    matrix             X               x                           ij               where             x                           ij              denotes the base logarithm of the CyCy            backgroundcorrected and normalized fluorescence            intensity ratio for gene             j in lymphoma sample             i The mean percentage of missing            observations per array is  and missing data were            inferred as outlined below The data were standardized            as described below                                Leukemia            The leukemia dataset is described in   and            available at   This dataset comes from a study of            gene expression in two types of acute leukemia acute            lymphoblastic leukemia ALL and acute myeloid leukemia            AML Geneexpression levels were measured using            Affymetrix highdensity oligonucleotide arrays            containing             p   human genes The data            comprise  cases of ALL  ALL Bcell and  ALL            Tcell and  cases of AML Following Golub             et al P Tamayo personal            communication three preprocessing steps were applied            to the normalized matrix of intensity values available            on the website after pooling the  mRNA samples from            the learning set and the  mRNA samples from the test            set First a floor of  and ceiling of  was            set second the data were filtered to exclude genes            with maxmin   or max  min   where max and            min refer respectively to the maximum and minimum            intensities for a particular gene across the  mRNA            samples and third the data were transformed to base             logarithms The data are then summarized by a               matrix             X               x                           ij               where             x                           ij              denotes the expression level for gene             j in mRNA sample             i There are no missing values            and the data were standardized as described below Note            that this standardization differs from the one            described in Golub             et al                                  NCI            In this study cDNA microarrays were used to examine            the variation in gene expression among the  cell            lines from the National Cancer Institutes NCI            anticancer drug screen    The cell lines were            derived from tumors with different sites of origin             breast  central nervous system CNS  colon             leukemia  melanoma  nonsmallcelllungcarcinoma            NSCLC  ovarian  prostate  renal and  unknown            ADRRES Gene expression was studied using            microarrays with  spotted DNA sequences In each            hybridization fluorescent cDNA targets were prepared            from a cellline mRNA sample redfluorescent dye Cy            and a reference mRNA sample obtained by pooling equal            mixtures of mRNA from  of the cell lines            greenfluorescent dye Cy To investigate the            reproducibility of the entire experimental procedure            cell culture mRNA isolation labeling hybridization            scanning and so on a leukemia K and a breast            cancer MCF cell line were analyzed by three            independent microarray experiments Ross             et al screened out genes with            missing data in more than two arrays In addition            because of their small class size the two prostate            cell lines and the unknown cell line ADRRES were            excluded from our analysis The data are summarized by            a    matrix             X               x                           ij               where             x                           ij              denotes the base logarithm of the CyCy            backgroundcorrected and normalized fluorescence            intensity ratio for gene             j in cell line             i The mean percentage of missing            observations per array is  and missing data were            inferred as outlined below The data were standardized            as described below                                Melanoma            The melanoma dataset is described in the recent            paper of Bittner             et al   and is available at              There are  melanoma samples and  control            samples Geneexpression levels were measured using            cDNA microarrays with  probe sequences            representing  unique genes In each hybridization            fluorescent cDNA targets were prepared from a melanoma            or control mRNA sample redfluorescent dye Cy and a            common reference mRNA sample greenfluorescent dye            Cy The following preprocessing steps were applied            by Bittner             et al First a gene was excluded            from the analysis if its average mean intensity above            background for the least intense signal Cy or Cy            across all experiments was   or its average spot            size across all experiments was   pixels and            second a floor and ceiling of  and             respectively were applied to the individual intensity            logratios This initial screening resulted in a            dataset of  genes see Supplemental Information to              document II page  Finally Bittner             et al did not include the seven            control samples in their analysis The data are            summarized by a    matrix             X               x                           ij               where             x                           ij              denotes the base logarithm of the CyCy            backgroundcorrected and normalized fluorescence            intensity ratio for gene             j in mRNA sample             i There were no             a priori known classes for this            dataset but the analysis of Bittner             et al suggests that two classes            may be present in the data with observations in one of            the classes Group A in their figures being more            tightly clustered There were no missing values and the            data were standardized as described below Note that            this standardization is slightly different from the one            described in                                      Imputation of missing data          For the lymphoma and NCI datasets each array          contains a number of genes with fluorescenceintensity          measurements that were flagged by the experimenter and          recorded as missing data points Missing data were          imputed by a simple           k nearestneighbor algorithm in          which the neighbors are the genes and the distance          between neighbors is based on the correlation between          their geneexpression levels across arrays For each gene          with missing data first compute its correlation with all          other           p   genes and then for each          missing array identify the           k nearest genes having data for          this array and infer the missing entry from the average          of the corresponding entries for the           k neighbors A value of           k   neighbors was used for the          lymphoma and NCI datasets For a detailed study of          methods for filling in missing values in microarray          experiments see   which suggests that a          nearestneighbor approach provides accurate and robust          estimates of missing values                          Standardization          The geneexpression data were standardized so that the          observations arrays have mean  and variance  across          variables genes Standardizing the data in this fashion          achieves a location and scale normalization of the          different arrays In a study of normalization methods we          have found scale adjustment to be desirable in some          cases to prevent the expression levels in one particular          array from dominating the average expression levels          across arrays   Furthermore this standardization is          consistent with the common practice in microarray          experiments of using the correlation between the          geneexpression profiles of two mRNA samples to measure          their similarity     In practice however we          recommend general adaptive and robust normalization          methods which correct for intensity spatial and other          types of dye biases using robust local regression                                     Preliminary gene selection          Expression levels were monitored for thousands of          genes in each of the four studies However the majority          of the genes exhibit nearconstant expression levels as          measured by the variance or coefficient of variation of          the expression levels across tumor samples Genes showing          nearly constant expression levels are not likely to be          useful for classification purposes therefore we chose          to exclude lowvariance genes from the clustering          process          Figure displays for each dataset the individual gene          variances divided by the maximum variance over all genes          All four variance curves show a sharp dropoff which          gradually flattens The plots are remarkably similar for          all the datasets with the melanoma dataset having the          fastest dropoff In this report the           p   most variable genes were          used to analyze the leukemia lymphoma and melanoma          datasets and the           p   most variable genes were          used for the NCI dataset as it contains more classes          Increasing the number of genes to           p   or decreasing the          number of genes to           p   did not have much effect on          the results data not shown One could also select genes          based on a coefficient of variation filter                          Correlation matrices                      Lymphoma            The existence of three wellseparated classes for            the lymphoma dataset is reflected in Figure for both            sets of genes the classes being more clearly separated            when the majority of the genes are screened out Recall            that geneexpression levels were measured using a            specialized cDNA microarray the Lymphochip enriched            in genes that are involved in the immune system This            may partly account for the clear separation of the            classes even when the correlation matrix is computed            using the full set of genes When PAM is applied to the            lymphoma dataset using the  genes with the largest            variance the             K      partitions are as            follows For             K   classes one cluster            consists of the FL and DLBCL classes combined and the            other consists of the CLL class This could reflect            differences in tissue sampling as the CLL mRNA samples            were obtained from peripheral blood cells as opposed to            lymphnode biopsy specimens for the FL and DLBCL            samples For             K   all three classes CLL            FL DLBCL are recovered as distinct clusters For             K   the largest DLBCL class is            divided into two clusters of approximately equal size            and the remaining two classes CLL and FL are            recovered as two distinct clusters The two DLBCL            clusters have a  overlap with the subclasses of            Alizadeh             et al   Finally for             K   the smallest class FL is            divided into two clusters and the rest of the clusters            are as with             K   On the basis of this            analysis we do not expect to recover more than four            classes in the lymphoma data                                Leukemia            Images of the correlation matrix for the leukemia            dataset are displayed in Figure  The three classes            corresponding to the ALL Tcell ALL Bcell and AML            samples clearly stand out in the image of the            correlation matrix for the  genes with the largest            variance but are indistinguishable in the image of the            correlation matrix based on all genes When the PAM            procedure is applied to the leukemia dataset using the             genes with the largest variance the results are as            follows For             K   eight ALL Tcell            observations are misclassified with the AML            observations For             K   classes one ALL Bcell            sample is clustered with the ALL Tcell tumors and the            rest of the observations are allocated correctly For             K   the ALL Bcell samples are            partitioned into two clusters Finally for             K   the AML samples are            partitioned into two clusters On the basis of the            correlation matrix one would expect to identify three            tumor classes in this dataset                                NCI            For the NCI cellline dataset the classes are not            clearly distinguishable from the images of the            correlation matrix Colon leukemia and melanoma cell            lines display the strongest correlations within class            whereas breast NSCLC and ovarian cell lines seem to be            the most heterogeneous classes When the PAM procedure            is applied to the NCI dataset using the  genes            with the largest variance and varying the number of            clusters             K   only five types of cell            lines tend to cluster together CNS colon leukemia            melanoma and renal cell lines On the basis of this            observation one should not expect to recover more than            five classes                                Melanoma            Finally for the melanoma dataset the image of the            correlation matrix for the             p   most variable genes            Figure  could possibly suggest the existence of a            subclass of tumors which includes the group A samples            of Bittner             et al   However some            observations in this cluster the first one from the            left in particular were not identified by Bittner             et al as being part of the tight            cluster Indeed when PAM is applied to the melanoma            dataset using the  genes with the largest variance            four additional observations are joined to the             observation cluster group A proposed by Bittner             et al Dividing the data into            three clusters results in a split of the             observations into two clusters One would expect to            identify at most two or three classes for this            dataset because of the small sample size                              